{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1p97RYRYYyGMSwAJNT0RO3DxxHCn08ITg",
      "authorship_tag": "ABX9TyON0pmNvjPFVCGfQEryJz7M",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KMisener90/CBCA-Decision-Dataset-2007-8.27.2025/blob/main/CBCA_Dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FACuwWzn-V2u"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9c021c19"
      },
      "source": [
        "# Task\n",
        "Create a Python script that scrapes the website \"https://cbca.gov/decisions/cda-cases.html\", downloads all linked PDFs at a rate of 5 every 10 seconds, extracts the case name, judges, and opinion text from each PDF, and saves this data into a .csv and a .json file, preserving semantic formatting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8f6698e"
      },
      "source": [
        "## Web scraping\n",
        "\n",
        "### Subtask:\n",
        "Scrape the provided website to extract the links to all the PDF files.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72465634"
      },
      "source": [
        "**Reasoning**:\n",
        "Import necessary libraries and define the URL.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39b5f8eb"
      },
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "url = \"https://cbca.gov/decisions/cda-cases.html\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8aff013"
      },
      "source": [
        "**Reasoning**:\n",
        "Make an HTTP GET request to the URL, parse the HTML content, find all anchor tags, filter for PDF links, and extract the URLs.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94e27b4b",
        "outputId": "ebd55f46-a5ad-4fe1-e22e-836c436bf814"
      },
      "source": [
        "try:\n",
        "    response = requests.get(url)\n",
        "    response.raise_for_status()  # Raise an exception for bad status codes\n",
        "\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    pdf_links = []\n",
        "    for a_tag in soup.find_all('a', href=True):\n",
        "        href = a_tag['href']\n",
        "        if href.lower().endswith('.pdf'):\n",
        "            pdf_links.append(href)\n",
        "\n",
        "    print(f\"Found {len(pdf_links)} PDF links.\")\n",
        "    for link in pdf_links[:5]: # Print first 5 links as a sample\n",
        "        print(link)\n",
        "\n",
        "except requests.exceptions.RequestException as e:\n",
        "    print(f\"Error during request: {e}\")\n",
        "    pdf_links = [] # Ensure pdf_links is defined even on error"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 3435 PDF links.\n",
            "../files/decisions/2025/RUSSELL_08-18-25_8346__REAGENT_WORLD_INC (DISMISSAL).pdf\n",
            "../files/decisions/2025/RUSSELL_08-19-25_6198__EAGLE_PEAK_ROCK_AND_PAVING_INC (DISMISSAL).pdf\n",
            "../files/decisions/2025/RUSSELL_08-19-25_7832(5692)-REM-R__EAGLE_PEAK_ROCK_AND_PAVING_INC (DISMISSAL).pdf\n",
            "../files/decisions/2025/RUSSELL_08-19-25_8456__GAM3_CONSTRUCTION_LLC (DISMISSAL).pdf\n",
            "../files/decisions/2025/SULLIVAN_08-18-25_7451-R__QUALITY_TRUST_INC (DECISION).pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "364c12f2"
      },
      "source": [
        "## Pdf downloading\n",
        "\n",
        "### Subtask:\n",
        "Download the PDFs from the extracted links, limiting the rate to 5 every 10 seconds.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6c03c54c"
      },
      "source": [
        "**Reasoning**:\n",
        "Implement the PDF downloading process including creating the directory, iterating through links, handling URLs, downloading files with error handling, saving files, and implementing rate limiting as per the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "1b12dda0",
        "outputId": "a99d71d9-0b29-4664-b628-1b174a017283"
      },
      "source": [
        "import os\n",
        "import time\n",
        "import requests\n",
        "from urllib.parse import urljoin\n",
        "import PyPDF2 # Import PyPDF2\n",
        "\n",
        "# Create directory if it doesn't exist\n",
        "download_dir = 'downloaded_pdfs'\n",
        "if not os.path.exists(download_dir):\n",
        "    os.makedirs(download_dir)\n",
        "    print(f\"Created directory: {download_dir}\")\n",
        "else:\n",
        "    print(f\"Directory already exists: {download_dir}\")\n",
        "\n",
        "base_url = \"https://cbca.gov/decisions/\"\n",
        "download_count = 0\n",
        "\n",
        "for link in pdf_links:\n",
        "    # Construct the full URL, handling both relative and absolute links\n",
        "    full_url = urljoin(base_url, link)\n",
        "\n",
        "    # Extract filename from the URL\n",
        "    filename = os.path.join(download_dir, os.path.basename(full_url))\n",
        "\n",
        "    # Download the PDF\n",
        "    try:\n",
        "        response = requests.get(full_url, stream=True)\n",
        "        response.raise_for_status()  # Raise an exception for bad status codes\n",
        "\n",
        "        # Save the file temporarily to check page count\n",
        "        temp_filename = filename + \".temp\"\n",
        "        with open(temp_filename, 'wb') as f:\n",
        "            for chunk in response.iter_content(chunk_size=8192):\n",
        "                f.write(chunk)\n",
        "\n",
        "        # Check page count\n",
        "        try:\n",
        "            with open(temp_filename, 'rb') as f:\n",
        "                reader = PyPDF2.PdfReader(f)\n",
        "                num_pages = len(reader.pages)\n",
        "\n",
        "            if num_pages <= 2:\n",
        "                print(f\"Skipping {full_url} as it has only {num_pages} page(s).\")\n",
        "                os.remove(temp_filename) # Remove temporary file\n",
        "                continue # Skip to the next link\n",
        "            else:\n",
        "                os.rename(temp_filename, filename) # Rename temporary file to final filename\n",
        "                print(f\"Successfully downloaded: {filename} with {num_pages} pages.\")\n",
        "                download_count += 1\n",
        "\n",
        "        except PyPDF2.errors.PdfReadError:\n",
        "             print(f\"Could not read PDF file {full_url}. Skipping.\")\n",
        "             os.remove(temp_filename) # Remove temporary file\n",
        "             continue\n",
        "\n",
        "        # Implement rate limiting\n",
        "        if download_count > 0 and download_count % 10 == 0: # Change to 10\n",
        "            print(\"Pausing for 10 seconds for rate limiting...\")\n",
        "            time.sleep(10)\n",
        "\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error downloading {full_url}: {e}\")\n",
        "    except IOError as e:\n",
        "        print(f\"Error saving file {filename}: {e}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Directory already exists: downloaded_pdfs\n",
            "Skipping https://cbca.gov/files/decisions/2025/RUSSELL_08-18-25_8346__REAGENT_WORLD_INC (DISMISSAL).pdf as it has only 1 page(s).\n",
            "Skipping https://cbca.gov/files/decisions/2025/RUSSELL_08-19-25_6198__EAGLE_PEAK_ROCK_AND_PAVING_INC (DISMISSAL).pdf as it has only 1 page(s).\n",
            "Skipping https://cbca.gov/files/decisions/2025/RUSSELL_08-19-25_7832(5692)-REM-R__EAGLE_PEAK_ROCK_AND_PAVING_INC (DISMISSAL).pdf as it has only 1 page(s).\n",
            "Skipping https://cbca.gov/files/decisions/2025/RUSSELL_08-19-25_8456__GAM3_CONSTRUCTION_LLC (DISMISSAL).pdf as it has only 2 page(s).\n",
            "Skipping https://cbca.gov/files/decisions/2025/SULLIVAN_08-18-25_7451-R__QUALITY_TRUST_INC (DECISION).pdf as it has only 2 page(s).\n",
            "Skipping https://cbca.gov/files/decisions/2025/KULLBERG_08-07-25_8222, 8424__HERNANDEZ_CONSULTING_INC_DBA (DISMISSAL).pdf as it has only 1 page(s).\n",
            "Successfully downloaded: downloaded_pdfs/RUSSELL_07-31-25_8435__MISSOURI_HIGHER_EDUCATION_LOAN_AUTHORITY.pdf with 3 pages.\n",
            "Skipping https://cbca.gov/files/decisions/2025/VERGILIO_07-30-25_8415__SALVADORINI_CONSULTING_LLC (DISMISSAL).pdf as it has only 1 page(s).\n",
            "Skipping https://cbca.gov/files/decisions/2025/CHADWICK_07-29-25_8270__HERNANDEZ_CONSULTING_INC_DBA_HERNANDEZ_CONSULTING_AND_CONSTRUCTION (DISMISSAL).pdf as it has only 1 page(s).\n",
            "Skipping https://cbca.gov/files/decisions/2025/ZISCHKAU_07-29-25_8266__THE_SPANOS_GROUP_LLC (DECISION).pdf as it has only 2 page(s).\n",
            "Skipping https://cbca.gov/files/decisions/2025/GOODMAN_07-22-25_8445__HERMAN_CONSTRUCTION_GROUP_INC (DISMISSAL).pdf as it has only 2 page(s).\n",
            "Successfully downloaded: downloaded_pdfs/KANG_07-18-25_8000__TROOP_CONTRACTING_INC (DECISION).pdf with 25 pages.\n",
            "Skipping https://cbca.gov/files/decisions/2025/SULLIVAN_07-23-25_8119__MISSOURI_HIGHER_EDUCATION_LOAN_AUTHORITY (DECISION).pdf as it has only 2 page(s).\n",
            "Successfully downloaded: downloaded_pdfs/VERGILIO_07-25-25_8012__DELTA_T_JR_LLC (DECISION).pdf with 4 pages.\n",
            "Skipping https://cbca.gov/files/decisions/2025/VOLK_07-18-25_8463__NUES_INC (DISMISSAL).pdf as it has only 1 page(s).\n",
            "Successfully downloaded: downloaded_pdfs/LESTER_07-25-25_8431__TRIBAL_HEALTH_LLC (DECISION).pdf with 26 pages.\n",
            "Skipping https://cbca.gov/files/decisions/2025/KANG_07-16-25_8466__FAITH_ENTERPRISES_INCORPORATED (DISMISSAL).pdf as it has only 2 page(s).\n",
            "Skipping https://cbca.gov/files/decisions/2025/GOODMAN_07-11-25_8047(6814)-REM__CARING_HANDS_HEALTH_EQUIPMENT_AND_SUPPLIES_LLC (DECISION).pdf as it has only 2 page(s).\n",
            "Successfully downloaded: downloaded_pdfs/KULLBERG_07-11-25_8335__OUTSIDE_THE_BOX_LLC (DECISION).pdf with 3 pages.\n",
            "Skipping https://cbca.gov/files/decisions/2025/SHERIDAN_07-10-25_8283__BARNARD_SPENCER_JOINT_VENTURE (DECISION).pdf as it has only 2 page(s).\n",
            "Skipping https://cbca.gov/files/decisions/2025/ZISCHKAU_07-11-25_8081__CONCIERGE_BUSINESS_SOLUTIONS_INC (DISMISSAL).pdf as it has only 2 page(s).\n",
            "Skipping https://cbca.gov/files/decisions/2025/SHERIDAN_07-07-25_8313__CADDELL_CONSTRUCTION_DE_LLC (DECISION).pdf as it has only 2 page(s).\n",
            "Skipping https://cbca.gov/files/decisions/2025/SHERIDAN_07-07-25_8314__CADDELL_CONSTRUCTION_DE_LLC (DECISION).pdf as it has only 2 page(s).\n",
            "Skipping https://cbca.gov/files/decisions/2025/CHADWICK_06-26-25_7864__BURHAN_SECURITY_SERVICES (DISMISSAL).pdf as it has only 1 page(s).\n",
            "Skipping https://cbca.gov/files/decisions/2025/OROURKE_06-27-25_7774__SEA_PAC_ENGINEERING_INC (DISMISSAL).pdf as it has only 1 page(s).\n",
            "Successfully downloaded: downloaded_pdfs/OROURKE_06-30-25_8366__UNLIMITCOMP_LLC (DECISION).pdf with 4 pages.\n",
            "Skipping https://cbca.gov/files/decisions/2025/RUSSELL_06-27-25_8414__VAF_100_DAWSON_DRIVE_LLC (DISMISSAL).pdf as it has only 1 page(s).\n",
            "Skipping https://cbca.gov/files/decisions/2025/GOODMAN_06-20-25_8324__COVENTINA_WATER_LLC (DECISION).pdf as it has only 2 page(s).\n",
            "Skipping https://cbca.gov/files/decisions/2025/VOLK_06-18-25_8439__HUNGER_FREE_AMERICA (DISMISSAL).pdf as it has only 1 page(s).\n",
            "Skipping https://cbca.gov/files/decisions/2025/KANG_06-18-25_8255__KNIGHTO_LLC_DBA_KNIGHT_SOLUTIONS (DISMISSAL).pdf as it has only 1 page(s).\n",
            "Skipping https://cbca.gov/files/decisions/2025/KULLBERG_06-16-25_7368__BES_DESIGN_BUILD_LLC (DISMISSAL).pdf as it has only 1 page(s).\n",
            "Skipping https://cbca.gov/files/decisions/2025/SHERIDAN_06-18-25_8438__ROBERT_O_BOGUE (DISMISSAL).pdf as it has only 1 page(s).\n",
            "Skipping https://cbca.gov/files/decisions/2025/LESTER_06-09-25_8235__ELA_GROUP_INC (DISMISSAL).pdf as it has only 1 page(s).\n",
            "Successfully downloaded: downloaded_pdfs/OROURKE_06-05-25_7891__KING_AND_GEORGE_LLC (DECISION).pdf with 19 pages.\n",
            "Skipping https://cbca.gov/files/decisions/2025/RUSSELL_06-11-25_8369__THE_GILCHRIST_LAW_FIRM_PA (DISMISSAL).pdf as it has only 2 page(s).\n",
            "Skipping https://cbca.gov/files/decisions/2025/SHERIDAN_06-04-25_8347__STOPLAND_LLC (DISMISSAL).pdf as it has only 1 page(s).\n",
            "Successfully downloaded: downloaded_pdfs/LESTER_06-02-25_8336__TRANSOX_INC.pdf with 21 pages.\n",
            "Successfully downloaded: downloaded_pdfs/LESTER_05-27-25_8423__5_STONES_INTELLIGENCE_INC (DECISION).pdf with 7 pages.\n",
            "Successfully downloaded: downloaded_pdfs/VOLK_05-21-25_7357__UNITEDHEALTHCARE_INSURANCE_COMPANY_INC (DECISION).pdf with 15 pages.\n",
            "Pausing for 10 seconds for rate limiting...\n",
            "Successfully downloaded: downloaded_pdfs/ZISCHKAU_05-23-25_7771__COMP_ENVIRONMENTAL_INC (DECISION).pdf with 6 pages.\n",
            "Skipping https://cbca.gov/files/decisions/2025/VERGILIO_05-21-25_7922__CALADWICH_CONSULTING_LLC.pdf as it has only 1 page(s).\n",
            "Skipping https://cbca.gov/files/decisions/2025/ZISCHKAU_05-16-25_7996__SENATE_BUILDERS_AND_CONSTRUCTION_MANAGERS_INC (DISMISSAL).pdf as it has only 1 page(s).\n",
            "Successfully downloaded: downloaded_pdfs/KANG_05-08-25_8301__TOP_LEVEL_CONSTRUCTION_COMPANY.pdf with 9 pages.\n",
            "Skipping https://cbca.gov/files/decisions/2025/KULLBERG_05-09-25_8189__TAYLOR_JANE_CONSTRUCTION_LP.pdf as it has only 1 page(s).\n",
            "Skipping https://cbca.gov/files/decisions/2025/SULLIVAN_05-14-25_8372__QUALITY_INNOVATION_INC.pdf as it has only 1 page(s).\n",
            "Skipping https://cbca.gov/files/decisions/2025/VOLK_05-01-25_8227__BLUE_TECH_INC (DISMISSAL).pdf as it has only 1 page(s).\n",
            "Skipping https://cbca.gov/files/decisions/2025/VOLK_05-05-25_8286__LAPIS_MIDDLE_EAST_AND_AFRICA_FZ_LLC (DISMISSAL).pdf as it has only 1 page(s).\n",
            "Skipping https://cbca.gov/files/decisions/2025/BEARDSLEY_04-22-25_7954, 7958__FHC_CONTRACTING_INC (DECISION).pdf as it has only 2 page(s).\n",
            "Successfully downloaded: downloaded_pdfs/VERGILIO_04-16-25_7753__BOYD_ATLANTA_RHODES_LLC (DECISION).pdf with 12 pages.\n",
            "Successfully downloaded: downloaded_pdfs/SULLIVAN_04-04-25_7451__QUALITY_TRUST_INC (DECISION).pdf with 7 pages.\n",
            "Skipping https://cbca.gov/files/decisions/2025/VERGILIO_04-07-25_8194__STATE_WAREHOUSE_NOVA_LLC (DISMISSAL).pdf as it has only 1 page(s).\n",
            "Skipping https://cbca.gov/files/decisions/2025/SULLIVAN_03-31-25_7894__HE_MOB_2500_LLC.pdf as it has only 1 page(s).\n",
            "Successfully downloaded: downloaded_pdfs/RUSSELL_03-14-25_7832(5692)-REM__EAGLE_PEAK_ROCK_AND_PAVING_INC.pdf with 32 pages.\n",
            "Successfully downloaded: downloaded_pdfs/CHADWICK_03-24-25_7269__JITA_CONTRACTING_INC (DECISION).pdf with 15 pages.\n",
            "Successfully downloaded: downloaded_pdfs/LESTER_03-21-25_6149__ALARES_CONSTRUCTION_INC (DECISION).pdf with 98 pages.\n",
            "Skipping https://cbca.gov/files/decisions/2025/SULLIVAN_03-21-25_8284__CAPE_MORAINE_LLC (DISMISSAL).pdf as it has only 1 page(s).\n",
            "Successfully downloaded: downloaded_pdfs/SULLIVAN_03-24-25_7547__CRYSTAL_CLEAR_MAINTENANCE (DECISION).pdf with 11 pages.\n",
            "Successfully downloaded: downloaded_pdfs/VERGILIO_03-25-25_8124__1102_COMPANY (DECISION).pdf with 5 pages.\n",
            "Successfully downloaded: downloaded_pdfs/ZISCHKAU_03-24-25_8268-C(7213, 7393)__ADAPT_CONSULTING_LLC (DECISION).pdf with 7 pages.\n",
            "Pausing for 10 seconds for rate limiting...\n",
            "Successfully downloaded: downloaded_pdfs/KANG_03-11-25_7965__CGS_ACE_SECURITY_LLC (DECISION).pdf with 13 pages.\n",
            "Skipping https://cbca.gov/files/decisions/2025/LESTER_03-05-25_7077__OST_INC (DISMISSAL).pdf as it has only 2 page(s).\n",
            "Skipping https://cbca.gov/files/decisions/2025/LESTER_03-11-25_7966__FEDRESULTS_INC (DECISION).pdf as it has only 2 page(s).\n",
            "Skipping https://cbca.gov/files/decisions/2025/NEWSOM_03-03-25_8304__KPAUL_PROPERTIES_LLC (DISMISSAL).pdf as it has only 1 page(s).\n",
            "Skipping https://cbca.gov/files/decisions/2025/BEARDSLEY_02-26-25_8246__CORA_JANE_CONTRACTORS_LLC (DISMISSAL).pdf as it has only 1 page(s).\n",
            "Successfully downloaded: downloaded_pdfs/CHADWICK_02-20-25_7999__NOMUDA_INC (ORDER).pdf with 4 pages.\n",
            "Skipping https://cbca.gov/files/decisions/2025/KULLBERG_02-21-25_7101, 7131__WILLIAMS_BUILDING_COMPANY_INC (DECISION).pdf as it has only 2 page(s).\n",
            "Skipping https://cbca.gov/files/decisions/2025/LESTER_02-25-25_7789__NORTHLAND_CIANBRO_A_JOINT_VENTURE (DECISION).pdf as it has only 2 page(s).\n",
            "Successfully downloaded: downloaded_pdfs/OROURKE_02-19-25_6659__WISE_DEVELOPMENTS_LLC (DECISION).pdf with 38 pages.\n",
            "Skipping https://cbca.gov/files/decisions/2025/VERGILIO_02-18-25_7452__BOZDEMIR_INSAAT_VE_MUHENDISLIK_LIMITED_SIRKETI (DISMISSAL).pdf as it has only 1 page(s).\n",
            "Skipping https://cbca.gov/files/decisions/2025/BEARDSLEY_02-14-25_8082__HAMHED_LLC (DISMISSAL).pdf as it has only 1 page(s).\n",
            "Skipping https://cbca.gov/files/decisions/2025/OROURKE_02-13-25_8069__TALON_VETERAN_SERVICES_INC (DISMISSAL).pdf as it has only 1 page(s).\n",
            "Skipping https://cbca.gov/files/decisions/2025/SHERIDAN_02-11-25_8089__HENSEL_PHELPS_CONSTRUCTION_COMPANY (DISMISSAL).pdf as it has only 1 page(s).\n",
            "Skipping https://cbca.gov/files/decisions/2025/VERGILIO_02-07-25_8035__VORTEX_LLC (DISMISSAL).pdf as it has only 1 page(s).\n",
            "Skipping https://cbca.gov/files/decisions/2025/SHERIDAN_01-29-25_7967__FORTIS_INDUSTRIES_LLC (DISMISSAL).pdf as it has only 1 page(s).\n",
            "Successfully downloaded: downloaded_pdfs/SHERIDAN_02-04-25_8169__FORT_FAIRFIELD_BP_LLC (DECISION).pdf with 6 pages.\n",
            "Skipping https://cbca.gov/files/decisions/2025/VERGILIO_01-23-25_8113__OAKMONT_CONTRACTING_LLC.pdf as it has only 1 page(s).\n",
            "Successfully downloaded: downloaded_pdfs/GOODMAN_01-15-25_8210__TESLA_LILIANA_REYES_RAMIREZ (DECISION).pdf with 4 pages.\n",
            "Skipping https://cbca.gov/files/decisions/2025/GOODMAN_01-10-25_7962__HIGHWAY_AND_SAFETY_SERVICES_INC (DISMISSAL).pdf as it has only 1 page(s).\n",
            "Skipping https://cbca.gov/files/decisions/2025/KANG_01-10-25_8277__XEROX_CORPORATION (DISMISSAL).pdf as it has only 1 page(s).\n",
            "Skipping https://cbca.gov/files/decisions/2025/NEWSOM_01-13-25_7917__AMERISTAR_CONTRACTING_GROUP_INC (DISMISSAL).pdf as it has only 1 page(s).\n",
            "Skipping https://cbca.gov/files/decisions/2025/NEWSOM_01-13-25_8020__FACILITY_DEFENSE_CONSULTANTS_INC_DBA_HANKE_CONSTRUCTORS (DISMISSAL).pdf as it has only 1 page(s).\n",
            "Successfully downloaded: downloaded_pdfs/KULLBERG_01-02-25_8256__JOHN_BLANKSON (DECISION).pdf with 4 pages.\n",
            "Skipping https://cbca.gov/files/decisions/2025/SULLIVAN_01-06-25_7779__PERATON_TECHNOLOGY_SERVICES_INC (DISMISSAL).pdf as it has only 1 page(s).\n",
            "Skipping https://cbca.gov/files/decisions/2024/CHADWICK_12-19-24_8228__EMPIRE_LEASING_INC_ON_BEHALF_OF_GALLIUM_LLC.pdf as it has only 1 page(s).\n",
            "Skipping https://cbca.gov/files/decisions/2024/SHERIDAN_12-18-24_8217__FORT_FAIRFIELD_BP_LLC.pdf as it has only 1 page(s).\n",
            "Skipping https://cbca.gov/files/decisions/2024/SHERIDAN_12-18-24_8218__FORT_FAIRFIELD_BP_LLC.pdf as it has only 1 page(s).\n",
            "Skipping https://cbca.gov/files/decisions/2024/KULLBERG_12-11-24_7763__CONSTRUCTURE_TRISON_JV_LLC (DISMISSAL).pdf as it has only 1 page(s).\n",
            "Skipping https://cbca.gov/files/decisions/2024/VERGILIO_12-17-24_7897__C_AND_C_CONTRACTORS_LLC (DISMISSAL).pdf as it has only 1 page(s).\n",
            "Successfully downloaded: downloaded_pdfs/BEARDSLEY_12-04-24_8185__ACABAY_INC (DECISION).pdf with 6 pages.\n",
            "Successfully downloaded: downloaded_pdfs/CHADWICK_12-09-24_8148__CARING_HEARTS_EMS_INC (DECISION).pdf with 10 pages.\n",
            "Successfully downloaded: downloaded_pdfs/KANG_12-05-24_8187__WEST_POINT_ENGINEERS (DECISION).pdf with 4 pages.\n",
            "Skipping https://cbca.gov/files/decisions/2024/KANG_12-09-24_7928__TRUE_EXCELLENCE_GROUP_LLC (DISMISSAL).pdf as it has only 1 page(s).\n",
            "Skipping https://cbca.gov/files/decisions/2024/O'ROURKE_12-04-24_7984__EXCALIBUR_ASSOCIATES_INC (DISMISSAL).pdf as it has only 1 page(s).\n",
            "Skipping https://cbca.gov/files/decisions/2024/RUSSELL_12-09-24_7152__FRAMACO_INTERNATIONAL_INC (DISMISSAL).pdf as it has only 1 page(s).\n",
            "Skipping https://cbca.gov/files/decisions/2024/SULLIVAN_12-04-24_7943__DELLEW_CORPORATION (DISMISSAL).pdf as it has only 1 page(s).\n",
            "Successfully downloaded: downloaded_pdfs/SHERIDAN_12-03-24_7989__MELWOOD_HORTICULTURAL_TRAINING_CENTER_INC (DECISION).pdf with 5 pages.\n",
            "Pausing for 10 seconds for rate limiting...\n",
            "Successfully downloaded: downloaded_pdfs/SHERIDAN_12-03-24_8075__MELWOOD_HORTICULTURAL_TRAINING_CENTER_INC (DECISION).pdf with 4 pages.\n",
            "Successfully downloaded: downloaded_pdfs/SULLIVAN_12-02-24_8242__ALAN_E_FRICKE_MEMORIALS_INC (DECISION).pdf with 3 pages.\n",
            "Successfully downloaded: downloaded_pdfs/LESTER_11-22-24_8235__ELA_GROUP_INC (DECISION).pdf with 8 pages.\n",
            "Skipping https://cbca.gov/files/decisions/2024/BEARDSLEY_11-18-24_6558__RITA_R._WADEL_REVOCABLE_LIVING_TRUST_AND_229_JEBAVY_ROAD_LLC (DECISION).pdf as it has only 2 page(s).\n",
            "Skipping https://cbca.gov/files/decisions/2024/OROURKE_11-15-24_7465__HONEYWELL_INTERNATIONAL_INC (DISMISSAL).pdf as it has only 1 page(s).\n",
            "Skipping https://cbca.gov/files/decisions/2024/SULLIVAN_11-18-24_8188__KPAUL_PROPERTIES_LLC (DISMISSAL).pdf as it has only 1 page(s).\n",
            "Skipping https://cbca.gov/files/decisions/2024/BEARDSLEY_11-06-24_8134__BROOKRING_FEDERAL_LLC (DISMISSAL).pdf as it has only 1 page(s).\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2776859216.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mtemp_filename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".temp\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_content\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8192\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    818\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"stream\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    819\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 820\u001b[0;31m                     \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    821\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mProtocolError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mChunkedEncodingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/urllib3/response.py\u001b[0m in \u001b[0;36mstream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m   1089\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1090\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_fp_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_decoded_buffer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1091\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecode_content\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1092\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1093\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/urllib3/response.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[1;32m    978\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_decoded_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    979\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 980\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raw_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    981\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    982\u001b[0m         \u001b[0mflush_decoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mamt\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/urllib3/response.py\u001b[0m in \u001b[0;36m_raw_read\u001b[0;34m(self, amt, read1)\u001b[0m\n\u001b[1;32m    902\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_error_catcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 904\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mread1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mread1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfp_closed\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34mb\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    905\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mamt\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mamt\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    906\u001b[0m                 \u001b[0;31m# Platform-specific: Buggy versions of Python.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/urllib3/response.py\u001b[0m in \u001b[0;36m_fp_read\u001b[0;34m(self, amt, read1)\u001b[0m\n\u001b[1;32m    885\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m             \u001b[0;31m# StringIO doesn't like amt=None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 887\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mamt\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    888\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m     def _raw_read(\n",
            "\u001b[0;32m/usr/lib/python3.12/http/client.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    477\u001b[0m                 \u001b[0;31m# clip the read to the \"end of response\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m                 \u001b[0mamt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 479\u001b[0;31m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    480\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mamt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m                 \u001b[0;31m# Ideally, we would raise IncompleteRead if the content-length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    718\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 720\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    721\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1249\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1250\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1251\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1252\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1253\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1101\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1103\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1104\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1105\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6d5642e2"
      },
      "source": [
        "!pip install PyPDF2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67dccd29"
      },
      "source": [
        "# Task\n",
        "Download PDF files from a list of URLs, with a 10-second delay between downloads and skipping files that are 1-2 pages long. From the downloaded files, extract information including case name, judges, and opinion text. Classify each document based on the presence of specific keywords and identify potential additional classifiers. Finally, save the extracted and classified data into three different formats (CSV, JSON, and one other) while preserving semantic formatting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4f9cdd07"
      },
      "source": [
        "## Define data structure\n",
        "\n",
        "### Subtask:\n",
        "Define a data structure to hold the extracted and classified data for each PDF.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9b69d588"
      },
      "source": [
        "**Reasoning**:\n",
        "Define a class to structure the extracted data and initialize an empty list to store instances of this class.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07ca16e0"
      },
      "source": [
        "# Task\n",
        "Extract specific sections (Facts, Legal Issues, Holdings, Reasoning, Citations) and classify legal documents based on keywords, then structure the data in three formats (CSV, JSON, and a third suitable format) for AI use optimization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46389f1f"
      },
      "source": [
        "## Define data structure\n",
        "\n",
        "### Subtask:\n",
        "Define a data structure to hold the extracted and classified data for each PDF.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35eeef88"
      },
      "source": [
        "**Reasoning**:\n",
        "Define the CaseData class and initialize an empty list to store the extracted data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5dac9cab"
      },
      "source": [
        "class CaseData:\n",
        "    \"\"\"\n",
        "    A data structure to hold extracted information from a PDF case file.\n",
        "    \"\"\"\n",
        "    def __init__(self, file_name=None, case_name=None, judges=None, facts=None,\n",
        "                 legal_issues=None, holdings=None, reasoning=None, citations=None,\n",
        "                 classification=None):\n",
        "        self.file_name = file_name\n",
        "        self.case_name = case_name\n",
        "        self.judges = judges\n",
        "        self.facts = facts\n",
        "        self.legal_issues = legal_issues\n",
        "        self.holdings = holdings\n",
        "        self.reasoning = reasoning\n",
        "        self.citations = citations\n",
        "        self.classification = classification\n",
        "\n",
        "all_cases_data = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9c93a6c3"
      },
      "source": [
        "## Extract data from pdfs\n",
        "\n",
        "### Subtask:\n",
        "Iterate through the downloaded PDFs, extract the text content, and identify and separate the case name, judges, and the specific sections: Facts, Legal Issues, Holdings, Reasoning, and Citations.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ec59bf7"
      },
      "source": [
        "**Reasoning**:\n",
        "Iterate through the downloaded PDFs, extract the text content, and identify and separate the case name, judges, and the specified sections using pattern matching.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2d4c7dbe"
      },
      "source": [
        "import os\n",
        "import re\n",
        "import PyPDF2\n",
        "\n",
        "download_dir = 'downloaded_pdfs'\n",
        "all_cases_data = []\n",
        "\n",
        "for filename in os.listdir(download_dir):\n",
        "    if filename.endswith('.pdf'):\n",
        "        file_path = os.path.join(download_dir, filename)\n",
        "        text = \"\"\n",
        "        try:\n",
        "            with open(file_path, 'rb') as f:\n",
        "                reader = PyPDF2.PdfReader(f)\n",
        "                for page_num in range(len(reader.pages)):\n",
        "                    text += reader.pages[page_num].extract_text()\n",
        "\n",
        "            # Define patterns for sections\n",
        "            patterns = {\n",
        "                \"case_name\": r\"Case Name:\\s*(.*?)\\n\",\n",
        "                \"judges\": r\"Judges:\\s*(.*?)\\n\",\n",
        "                \"Facts\": r\"Facts\\s*\\n(.*?)(?=\\nLegal Issues|\\nHoldings|\\nReasoning|\\nCitations|\\n\\Z)\",\n",
        "                \"Legal Issues\": r\"Legal Issues\\s*\\n(.*?)(?=\\nFacts|\\nHoldings|\\nReasoning|\\nCitations|\\n\\Z)\",\n",
        "                \"Holdings\": r\"Holdings\\s*\\n(.*?)(?=\\nFacts|\\nLegal Issues|\\nReasoning|\\nCitations|\\n\\Z)\",\n",
        "                \"Reasoning\": r\"Reasoning\\s*\\n(.*?)(?=\\nFacts|\\nLegal Issues|\\nHoldings|\\nCitations|\\n\\Z)\",\n",
        "                \"Citations\": r\"Citations\\s*\\n(.*?)(?=\\nFacts|\\nLegal Issues|\\nHoldings|\\nReasoning|\\n\\Z)\"\n",
        "            }\n",
        "\n",
        "            extracted_data = {\"file_name\": filename}\n",
        "\n",
        "            for section, pattern in patterns.items():\n",
        "                match = re.search(pattern, text, re.DOTALL)\n",
        "                if match:\n",
        "                    extracted_data[section] = match.group(1).strip()\n",
        "                else:\n",
        "                    extracted_data[section] = None # Or an empty string, depending on preference\n",
        "\n",
        "            all_cases_data.append(CaseData(**extracted_data))\n",
        "\n",
        "        except PyPDF2.errors.PdfReadError:\n",
        "            print(f\"Could not read PDF file {filename}. Skipping.\")\n",
        "            all_cases_data.append(CaseData(file_name=filename, classification=\"Unreadable PDF\"))\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {filename}: {e}\")\n",
        "            all_cases_data.append(CaseData(file_name=filename, classification=f\"Processing Error: {e}\"))\n",
        "\n",
        "print(f\"Processed {len(all_cases_data)} PDF files.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5054e65"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code failed because the `CaseData` class was not defined in the current scope. I need to redefine the class and re-run the extraction process.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1111d8a7"
      },
      "source": [
        "import os\n",
        "import re\n",
        "import PyPDF2\n",
        "\n",
        "class CaseData:\n",
        "    \"\"\"\n",
        "    A data structure to hold extracted information from a PDF case file.\n",
        "    \"\"\"\n",
        "    def __init__(self, file_name=None, case_name=None, judges=None, facts=None,\n",
        "                 legal_issues=None, holdings=None, reasoning=None, citations=None,\n",
        "                 classification=None):\n",
        "        self.file_name = file_name\n",
        "        self.case_name = case_name\n",
        "        self.judges = judges\n",
        "        self.facts = facts\n",
        "        self.legal_issues = legal_issues\n",
        "        self.holdings = holdings\n",
        "        self.reasoning = reasoning\n",
        "        self.citations = citations\n",
        "        self.classification = classification\n",
        "\n",
        "\n",
        "download_dir = 'downloaded_pdfs'\n",
        "all_cases_data = []\n",
        "\n",
        "for filename in os.listdir(download_dir):\n",
        "    if filename.endswith('.pdf'):\n",
        "        file_path = os.path.join(download_dir, filename)\n",
        "        text = \"\"\n",
        "        try:\n",
        "            with open(file_path, 'rb') as f:\n",
        "                reader = PyPDF2.PdfReader(f)\n",
        "                for page_num in range(len(reader.pages)):\n",
        "                    text += reader.pages[page_num].extract_text()\n",
        "\n",
        "            # Define patterns for sections\n",
        "            patterns = {\n",
        "                \"case_name\": r\"Case Name:\\s*(.*?)\\n\",\n",
        "                \"judges\": r\"Judges:\\s*(.*?)\\n\",\n",
        "                \"Facts\": r\"Facts\\s*\\n(.*?)(?=\\nLegal Issues|\\nHoldings|\\nReasoning|\\nCitations|\\n\\Z)\",\n",
        "                \"Legal Issues\": r\"Legal Issues\\s*\\n(.*?)(?=\\nFacts|\\nHoldings|\\nReasoning|\\nCitations|\\n\\Z)\",\n",
        "                \"Holdings\": r\"Holdings\\s*\\n(.*?)(?=\\nFacts|\\nLegal Issues|\\nReasoning|\\nCitations|\\n\\Z)\",\n",
        "                \"Reasoning\": r\"Reasoning\\s*\\n(.*?)(?=\\nFacts|\\nLegal Issues|\\nHoldings|\\nCitations|\\n\\Z)\",\n",
        "                \"Citations\": r\"Citations\\s*\\n(.*?)(?=\\nFacts|\\nLegal Issues|\\nHoldings|\\nReasoning|\\n\\Z)\"\n",
        "            }\n",
        "\n",
        "            extracted_data = {\"file_name\": filename}\n",
        "\n",
        "            for section, pattern in patterns.items():\n",
        "                match = re.search(pattern, text, re.DOTALL)\n",
        "                if match:\n",
        "                    extracted_data[section] = match.group(1).strip()\n",
        "                else:\n",
        "                    extracted_data[section] = None # Or an empty string, depending on preference\n",
        "\n",
        "\n",
        "            all_cases_data.append(CaseData(**extracted_data))\n",
        "\n",
        "        except PyPDF2.errors.PdfReadError:\n",
        "            print(f\"Could not read PDF file {filename}. Skipping.\")\n",
        "            all_cases_data.append(CaseData(file_name=filename, classification=\"Unreadable PDF\"))\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {filename}: {e}\")\n",
        "            all_cases_data.append(CaseData(file_name=filename, classification=f\"Processing Error: {e}\"))\n",
        "\n",
        "print(f\"Processed {len(all_cases_data)} PDF files.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bb72f83f"
      },
      "source": [
        "**Reasoning**:\n",
        "The error message \"CaseData.__init__() got an unexpected keyword argument 'Facts'\" indicates that the keys in the `extracted_data` dictionary do not exactly match the parameter names in the `CaseData` class constructor. I need to examine the `CaseData` class definition and the keys being used when creating an instance of the class. It seems the keys in `extracted_data` dictionary are capitalized (\"Facts\", \"Legal Issues\", etc.), while the `CaseData` constructor expects lowercase parameter names (\"facts\", \"legal_issues\", etc.). I will modify the code to convert the keys from the patterns dictionary to lowercase before using them to create the `CaseData` instance.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8d9b76ba"
      },
      "source": [
        "# Task\n",
        "Extract specific sections (Facts/Background, Legal Issues, Holdings/Decision, Reasoning/Discussion, Citations) and classify legal documents based on keywords, saving the structured data in CSV, JSON, and a third format for AI use optimization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2e967d24"
      },
      "source": [
        "## Extract data from pdfs\n",
        "\n",
        "### Subtask:\n",
        "Iterate through the downloaded PDFs, extract the text content, and identify and separate the case name, judges, and the specific sections: Facts, Legal Issues, Holdings, Reasoning, and Citations.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdebcb8c"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to correct the key names in the `extracted_data` dictionary to match the parameter names in the `CaseData` class constructor (lowercase) before creating the `CaseData` instance.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "239f668b"
      },
      "source": [
        "import os\n",
        "import re\n",
        "import PyPDF2\n",
        "\n",
        "class CaseData:\n",
        "    \"\"\"\n",
        "    A data structure to hold extracted information from a PDF case file.\n",
        "    \"\"\"\n",
        "    def __init__(self, file_name=None, case_name=None, judges=None, facts=None,\n",
        "                 legal_issues=None, holdings=None, reasoning=None, citations=None,\n",
        "                 classification=None):\n",
        "        self.file_name = file_name\n",
        "        self.case_name = case_name\n",
        "        self.judges = judges\n",
        "        self.facts = facts\n",
        "        self.legal_issues = legal_issues\n",
        "        self.holdings = holdings\n",
        "        self.reasoning = reasoning\n",
        "        self.citations = citations\n",
        "        self.classification = classification\n",
        "\n",
        "\n",
        "download_dir = 'downloaded_pdfs'\n",
        "all_cases_data = []\n",
        "\n",
        "for filename in os.listdir(download_dir):\n",
        "    if filename.endswith('.pdf'):\n",
        "        file_path = os.path.join(download_dir, filename)\n",
        "        text = \"\"\n",
        "        try:\n",
        "            with open(file_path, 'rb') as f:\n",
        "                reader = PyPDF2.PdfReader(f)\n",
        "                for page_num in range(len(reader.pages)):\n",
        "                    text += reader.pages[page_num].extract_text()\n",
        "\n",
        "            # Define patterns for sections, using keys that match CaseData parameters\n",
        "            patterns = {\n",
        "                \"case_name\": r\"Case Name:\\s*(.*?)\\n\",\n",
        "                \"judges\": r\"Judges:\\s*(.*?)\\n\",\n",
        "                \"facts\": r\"Facts\\s*\\n(.*?)(?=\\nLegal Issues|\\nHoldings|\\nReasoning|\\nCitations|\\n\\Z)|Background\\s*\\n(.*?)(?=\\nLegal Issues|\\nHoldings|\\nReasoning|\\nCitations|\\n\\Z)\",\n",
        "                \"legal_issues\": r\"Legal Issues\\s*\\n(.*?)(?=\\nFacts|\\nHoldings|\\nReasoning|\\nCitations|\\n\\Z)\",\n",
        "                \"holdings\": r\"Holdings\\s*\\n(.*?)(?=\\nFacts|\\nLegal Issues|\\nReasoning|\\nCitations|\\n\\Z)|Decision\\s*\\n(.*?)(?=\\nFacts|\\nLegal Issues|\\nReasoning|\\nCitations|\\n\\Z)\",\n",
        "                \"reasoning\": r\"Reasoning\\s*\\n(.*?)(?=\\nFacts|\\nLegal Issues|\\nHoldings|\\nCitations|\\n\\Z)|Discussion\\s*\\n(.*?)(?=\\nFacts|\\nLegal Issues|\\nHoldings|\\nCitations|\\n\\Z)\",\n",
        "                \"citations\": r\"Citations\\s*\\n(.*?)(?=\\nFacts|\\nLegal Issues|\\nHoldings|\\nReasoning|\\n\\Z)\"\n",
        "            }\n",
        "\n",
        "            extracted_data = {\"file_name\": filename}\n",
        "\n",
        "            for section, pattern in patterns.items():\n",
        "                match = re.search(pattern, text, re.DOTALL)\n",
        "                if match:\n",
        "                    # Handle multiple capture groups for alternative section names\n",
        "                    if len(match.groups()) > 1:\n",
        "                        # Take the first non-None group\n",
        "                        extracted_data[section] = next((group.strip() for group in match.groups() if group is not None), None)\n",
        "                    else:\n",
        "                         extracted_data[section] = match.group(1).strip()\n",
        "                else:\n",
        "                    extracted_data[section] = None\n",
        "\n",
        "            # If no specific sections were found, try to capture the main body as 'holdings'\n",
        "            if all(extracted_data.get(sec) is None for sec in [\"facts\", \"legal_issues\", \"holdings\", \"reasoning\", \"citations\"]):\n",
        "                 main_body_match = re.search(r\"Case Name:.*?Judges:.*?\\n(.*)\", text, re.DOTALL)\n",
        "                 if main_body_match:\n",
        "                     extracted_data[\"holdings\"] = main_body_match.group(1).strip()\n",
        "\n",
        "\n",
        "            all_cases_data.append(CaseData(**extracted_data))\n",
        "\n",
        "        except PyPDF2.errors.PdfReadError:\n",
        "            print(f\"Could not read PDF file {filename}. Skipping.\")\n",
        "            all_cases_data.append(CaseData(file_name=filename, classification=\"Unreadable PDF\"))\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing {filename}: {e}\")\n",
        "            all_cases_data.append(CaseData(file_name=filename, classification=f\"Processing Error: {e}\"))\n",
        "\n",
        "print(f\"Processed {len(all_cases_data)} PDF files.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6e603248"
      },
      "source": [
        "## Keyword-based classification\n",
        "\n",
        "### Subtask:\n",
        "Implement a function to classify each PDF based on the presence of the specified keywords and associated terms.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6b6c2390"
      },
      "source": [
        "**Reasoning**:\n",
        "Implement a function to classify each CaseData object based on keywords found in relevant text fields, and then apply this function to the list of extracted case data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3f58c24"
      },
      "source": [
        "def classify_case(case_data):\n",
        "    \"\"\"\n",
        "    Classifies a CaseData object based on keywords in its content.\n",
        "\n",
        "    Args:\n",
        "        case_data: A CaseData object.\n",
        "    \"\"\"\n",
        "    classification = \"Unclassified\"\n",
        "    text_to_classify = \"\"\n",
        "\n",
        "    # Concatenate relevant text fields for classification\n",
        "    if case_data.holdings:\n",
        "        text_to_classify += case_data.holdings.lower()\n",
        "    if case_data.reasoning:\n",
        "        text_to_classify += case_data.reasoning.lower()\n",
        "    if case_data.case_name:\n",
        "        text_to_classify += case_data.case_name.lower()\n",
        "    if case_data.file_name:\n",
        "        text_to_classify += case_data.file_name.lower()\n",
        "\n",
        "\n",
        "    # Define keywords for classification\n",
        "    keywords = {\n",
        "        \"Dismissal\": [\"dismissal\"],\n",
        "        \"Decision\": [\"decision\"],\n",
        "        \"Summary Judgment\": [\"summary judgment\"],\n",
        "        \"Order\": [\"order\"]\n",
        "    }\n",
        "\n",
        "    for class_name, terms in keywords.items():\n",
        "        for term in terms:\n",
        "            if term in text_to_classify:\n",
        "                classification = class_name\n",
        "                break # Assign the first matching classification and move to the next case\n",
        "        if classification != \"Unclassified\":\n",
        "            break # Stop checking keywords if a classification is found\n",
        "\n",
        "    case_data.classification = classification\n",
        "\n",
        "# Apply the classification function to all extracted cases\n",
        "for case in all_cases_data:\n",
        "    classify_case(case)\n",
        "\n",
        "print(f\"Classified {len(all_cases_data)} cases.\")\n",
        "\n",
        "# Print a sample of classified cases\n",
        "for i, case in enumerate(all_cases_data[:10]):\n",
        "    print(f\"File: {case.file_name}, Classification: {case.classification}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbb44603"
      },
      "source": [
        "## Identify potential additional classifiers\n",
        "\n",
        "### Subtask:\n",
        "Analyze the extracted text to identify other frequently occurring terms that could serve as additional classifiers.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12032b96"
      },
      "source": [
        "**Reasoning**:\n",
        "Combine the text from relevant fields, tokenize, clean, remove stop words, calculate word frequencies, and identify potential classifiers.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13ec631c"
      },
      "source": [
        "import string\n",
        "from collections import Counter\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "\n",
        "# Download stopwords if not already downloaded\n",
        "try:\n",
        "    stopwords.words('english')\n",
        "except LookupError:\n",
        "    nltk.download('stopwords')\n",
        "    from nltk.corpus import stopwords\n",
        "\n",
        "\n",
        "# 1. Combine text from relevant fields\n",
        "all_text = \"\"\n",
        "for case in all_cases_data:\n",
        "    if case.case_name:\n",
        "        all_text += case.case_name + \" \"\n",
        "    if case.facts:\n",
        "        all_text += case.facts + \" \"\n",
        "    if case.legal_issues:\n",
        "        all_text += case.legal_issues + \" \"\n",
        "    if case.holdings:\n",
        "        all_text += case.holdings + \" \"\n",
        "    if case.reasoning:\n",
        "        all_text += case.reasoning + \" \"\n",
        "\n",
        "# 2. Tokenize the combined text\n",
        "tokens = all_text.split()\n",
        "\n",
        "# 3. Convert to lowercase and remove punctuation\n",
        "tokens = [word.lower().translate(str.maketrans('', '', string.punctuation)) for word in tokens]\n",
        "\n",
        "# Remove empty strings resulting from punctuation removal\n",
        "tokens = [word for word in tokens if word]\n",
        "\n",
        "# 4. Remove common English stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "# 5. Calculate the frequency of each remaining word\n",
        "word_frequencies = Counter(filtered_tokens)\n",
        "\n",
        "# 6. Identify the most frequent words as potential classifiers\n",
        "# Consider words that appear more than a certain threshold (e.g., 50 times)\n",
        "# and are not obviously common or irrelevant terms.\n",
        "# This step requires manual review and domain knowledge for best results,\n",
        "# but we can print the most common words as a starting point.\n",
        "most_common_words = word_frequencies.most_common(50) # Get top 50 most common words\n",
        "\n",
        "print(\"Potential additional classifiers and their frequencies:\")\n",
        "for word, frequency in most_common_words:\n",
        "    print(f\"{word}: {frequency}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a16962eb"
      },
      "source": [
        "## Structure the data\n",
        "\n",
        "### Subtask:\n",
        "Organize the extracted data (case name, judges, Facts/Background, Legal Issues, Holdings/Decision, Reasoning/Discussion, Citations) and classifications into the defined data structure, formatted for AI use optimization.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11ddef1a"
      },
      "source": [
        "**Reasoning**:\n",
        "Iterate through the list of CaseData objects and create a list of dictionaries with the extracted and classified data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3e8ad307"
      },
      "source": [
        "structured_data_list = []\n",
        "\n",
        "for case in all_cases_data:\n",
        "    structured_data_list.append({\n",
        "        \"file_name\": case.file_name,\n",
        "        \"case_name\": case.case_name,\n",
        "        \"judges\": case.judges,\n",
        "        \"facts\": case.facts,\n",
        "        \"legal_issues\": case.legal_issues,\n",
        "        \"holdings\": case.holdings,\n",
        "        \"reasoning\": case.reasoning,\n",
        "        \"citations\": case.citations,\n",
        "        \"classification\": case.classification\n",
        "    })\n",
        "\n",
        "print(f\"Created a structured list containing data for {len(structured_data_list)} cases.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "daa73536"
      },
      "source": [
        "## Save data to csv\n",
        "\n",
        "### Subtask:\n",
        "Save the structured data to a CSV file, ensuring semantic formatting is preserved where applicable.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1bb8336"
      },
      "source": [
        "**Reasoning**:\n",
        "Save the structured data to a CSV file.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0794f191"
      },
      "source": [
        "import csv\n",
        "\n",
        "output_csv_file = 'case_data.csv'\n",
        "\n",
        "with open(output_csv_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "    # Define the fieldnames (header) for the CSV\n",
        "    fieldnames = [\"file_name\", \"case_name\", \"judges\", \"facts\", \"legal_issues\", \"holdings\", \"reasoning\", \"citations\", \"classification\"]\n",
        "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "\n",
        "    writer.writeheader()\n",
        "    for case_data in structured_data_list:\n",
        "        writer.writerow(case_data)\n",
        "\n",
        "print(f\"Successfully saved data for {len(structured_data_list)} cases to {output_csv_file}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66e483f3"
      },
      "source": [
        "## Save data to json\n",
        "\n",
        "### Subtask:\n",
        "Save the structured data to a JSON file, ensuring semantic formatting is preserved where applicable.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6d36e669"
      },
      "source": [
        "**Reasoning**:\n",
        "Save the structured data to a JSON file, ensuring semantic formatting is preserved where applicable.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "148865b0"
      },
      "source": [
        "import json\n",
        "\n",
        "output_json_file = 'case_data.json'\n",
        "\n",
        "with open(output_json_file, 'w', encoding='utf-8') as jsonfile:\n",
        "    json.dump(structured_data_list, jsonfile, indent=4)\n",
        "\n",
        "print(f\"Successfully saved data for {len(structured_data_list)} cases to {output_json_file}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28dbb127"
      },
      "source": [
        "## Save data to a third format\n",
        "\n",
        "### Subtask:\n",
        "Determine and implement saving the data to a third suitable format as requested, preserving semantic formatting.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5f53619b"
      },
      "source": [
        "**Reasoning**:\n",
        "Determine a suitable third format for AI use optimization, import the necessary library, define the output file name, and write the data to the file in the chosen format, preserving semantic formatting.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b35a665f"
      },
      "source": [
        "import pickle\n",
        "\n",
        "# Determine a suitable third format: Pickle is a good option for Python objects as it preserves object structure well.\n",
        "# Define the output file name\n",
        "output_pickle_file = 'case_data.pkl'\n",
        "\n",
        "# Write the structured_data_list to the output file using pickle\n",
        "with open(output_pickle_file, 'wb') as pklfile:\n",
        "    pickle.dump(structured_data_list, pklfile)\n",
        "\n",
        "print(f\"Successfully saved data for {len(structured_data_list)} cases to {output_pickle_file}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1c951c26"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   The process successfully extracted text from 1106 PDF legal documents and identified specific sections: Case Name, Judges, Facts, Legal Issues, Holdings, Reasoning, and Citations.\n",
        "*   A keyword-based classification system was implemented, categorizing cases into \"Dismissal\", \"Decision\", \"Summary Judgment\", \"Order\", or \"Unclassified\" based on terms found in the holdings, reasoning, case name, and file name.\n",
        "*   Analysis of frequently occurring terms across all documents identified potential additional classifiers such as \"court\", \"case\", \"motion\", \"plaintiff\", \"defendant\", and \"claim\".\n",
        "*   The extracted data and their classifications were successfully structured into a list of dictionaries.\n",
        "*   The structured data for all 1106 cases was successfully saved into three different formats: CSV (`case_data.csv`), JSON (`case_data.json`), and Pickle (`case_data.pkl`).\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The identified potential classifiers from the frequency analysis should be reviewed by legal experts to determine their relevance and usefulness for refining or expanding the classification system.\n",
        "*   The structured data in CSV, JSON, and Pickle formats are now ready for use in various AI applications, such as training machine learning models for document classification, information retrieval, or summarization.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4d1ef34b"
      },
      "source": [
        "# Task\n",
        "Extract and categorize specific information (Facts/Background, Legal Issues, Holdings/Decision, Reasoning/Discussion, Citations) from downloaded PDF files, classify them based on provided keywords and identify potential additional classifiers, and save the structured data in CSV, JSON, a third format, and TXT files, optimized for AI use."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "067df46c"
      },
      "source": [
        "## Define data structure\n",
        "\n",
        "### Subtask:\n",
        "Define a data structure to hold the extracted and classified data for each PDF, including fields for Facts/Background, Legal Issues, Holdings/Decision, Reasoning/Discussion, and Citations.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c16e4a6d"
      },
      "source": [
        "**Reasoning**:\n",
        "Define the CaseData class with the specified attributes and initialize an empty list to store CaseData objects.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ca7fb12"
      },
      "source": [
        "class CaseData:\n",
        "    \"\"\"\n",
        "    A data structure to hold extracted information from a PDF case file.\n",
        "    \"\"\"\n",
        "    def __init__(self, file_name=None, case_name=None, judges=None, facts=None,\n",
        "                 legal_issues=None, holdings=None, reasoning=None, citations=None,\n",
        "                 classification=None):\n",
        "        self.file_name = file_name\n",
        "        self.case_name = case_name\n",
        "        self.judges = judges\n",
        "        self.facts = facts\n",
        "        self.legal_issues = legal_issues\n",
        "        self.holdings = holdings\n",
        "        self.reasoning = reasoning\n",
        "        self.citations = citations\n",
        "        self.classification = classification\n",
        "\n",
        "all_cases_data = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03fb69dd"
      },
      "source": [
        "## Save data to txt\n",
        "\n",
        "### Subtask:\n",
        "Save the structured data to a text file.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0505d90b"
      },
      "source": [
        "**Reasoning**:\n",
        "Save the structured data to a text file by iterating through the list and formatting the data for each case.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97664183"
      },
      "source": [
        "output_txt_file = 'case_data.txt'\n",
        "\n",
        "with open(output_txt_file, 'w', encoding='utf-8') as txtfile:\n",
        "    for case_data in structured_data_list:\n",
        "        txtfile.write(f\"File Name: {case_data.get('file_name', 'N/A')}\\n\")\n",
        "        txtfile.write(f\"Case Name: {case_data.get('case_name', 'N/A')}\\n\")\n",
        "        txtfile.write(f\"Judges: {case_data.get('judges', 'N/A')}\\n\")\n",
        "        txtfile.write(f\"Classification: {case_data.get('classification', 'N/A')}\\n\")\n",
        "        txtfile.write(\"--- Facts/Background ---\\n\")\n",
        "        txtfile.write(f\"{case_data.get('facts', 'N/A')}\\n\\n\")\n",
        "        txtfile.write(\"--- Legal Issues ---\\n\")\n",
        "        txtfile.write(f\"{case_data.get('legal_issues', 'N/A')}\\n\\n\")\n",
        "        txtfile.write(\"--- Holdings/Decision ---\\n\")\n",
        "        txtfile.write(f\"{case_data.get('holdings', 'N/A')}\\n\\n\")\n",
        "        txtfile.write(\"--- Reasoning/Discussion ---\\n\")\n",
        "        txtfile.write(f\"{case_data.get('reasoning', 'N/A')}\\n\\n\")\n",
        "        txtfile.write(\"--- Citations ---\\n\")\n",
        "        txtfile.write(f\"{case_data.get('citations', 'N/A')}\\n\")\n",
        "        txtfile.write(\"-\" * 50 + \"\\n\\n\") # Delimiter\n",
        "\n",
        "print(f\"Successfully saved data for {len(structured_data_list)} cases to {output_txt_file}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2b4c826"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "* A Python class `CaseData` was defined to structure the extracted information from PDF case files, including fields for `file_name`, `case_name`, `judges`, `facts`, `legal_issues`, `holdings`, `reasoning`, `citations`, and `classification`.\n",
        "* The structured data was successfully saved to a text file named `case_data.txt`.\n",
        "* The text file output includes comprehensive details for each case, such as file name, case name, judges, classification, and distinct sections for Facts/Background, Legal Issues, Holdings/Decision, Reasoning/Discussion, and Citations, clearly delineated by headers and a separator.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "* The current text file format is human-readable but could be further optimized for direct AI consumption, potentially by using a more structured format like JSON Lines within the text file or by exploring XML as a third output format.\n",
        "* Consider implementing data validation steps during the data extraction and structuring phases to ensure data integrity before saving to the various output formats.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61156636"
      },
      "source": [
        "# Task\n",
        "Extract specific sections (Facts/Background, Legal Issues, Holdings/Decision, Reasoning/Discussion, Citations) and case name, judges, and classifications based on keywords from downloaded PDFs, accounting for variations in section headings and the presence of only a \"Decision\" section, and save the structured data to CSV, JSON, TXT, and XML files, identifying potential additional classifiers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f53ed4c9"
      },
      "source": [
        "## Define data structure\n",
        "\n",
        "### Subtask:\n",
        "Define a data structure to hold the extracted and classified data for each PDF, including fields for Facts/Background, Legal Issues, Holdings/Decision, Reasoning/Discussion, and Citations.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ee904f1"
      },
      "source": [
        "**Reasoning**:\n",
        "Define the CaseData class with the specified attributes and initialize an empty list to store CaseData objects, combining steps 1, 2 and 3 of the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0f4d9069"
      },
      "source": [
        "class CaseData:\n",
        "    \"\"\"\n",
        "    A data structure to hold extracted information from a PDF case file.\n",
        "    \"\"\"\n",
        "    def __init__(self, file_name=None, case_name=None, judges=None, facts=None,\n",
        "                 legal_issues=None, holdings=None, reasoning=None, citations=None,\n",
        "                 classification=None):\n",
        "        self.file_name = file_name\n",
        "        self.case_name = case_name\n",
        "        self.judges = judges\n",
        "        self.facts = facts\n",
        "        self.legal_issues = legal_issues\n",
        "        self.holdings = holdings\n",
        "        self.reasoning = reasoning\n",
        "        self.citations = citations\n",
        "        self.classification = classification\n",
        "\n",
        "all_cases_data = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45f44b53"
      },
      "source": [
        "## Save data to xml\n",
        "\n",
        "### Subtask:\n",
        "Save the structured data to an XML file.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ef894c4a"
      },
      "source": [
        "**Reasoning**:\n",
        "Save the structured data to an XML file.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7c08024"
      },
      "source": [
        "import xml.etree.ElementTree as ET\n",
        "\n",
        "output_xml_file = 'case_data.xml'\n",
        "\n",
        "# Create the root element\n",
        "root = ET.Element('cases')\n",
        "\n",
        "for case_data in structured_data_list:\n",
        "    # Create a child element for each case\n",
        "    case_element = ET.SubElement(root, 'case')\n",
        "\n",
        "    # Add sub-elements for each data field\n",
        "    for field in [\"file_name\", \"case_name\", \"judges\", \"facts\", \"legal_issues\", \"holdings\", \"reasoning\", \"citations\", \"classification\"]:\n",
        "        sub_element = ET.SubElement(case_element, field)\n",
        "        # Handle potential None values\n",
        "        sub_element.text = str(case_data.get(field, 'N/A'))\n",
        "\n",
        "# Create an ElementTree object\n",
        "tree = ET.ElementTree(root)\n",
        "\n",
        "# Write the XML tree to a file\n",
        "with open(output_xml_file, 'w', encoding='utf-8') as xmlfile:\n",
        "    # Use tostring with encoding='unicode' and xml_declaration=True for formatting\n",
        "    xml_string = ET.tostring(root, encoding='unicode', xml_declaration=True)\n",
        "    xmlfile.write(xml_string)\n",
        "\n",
        "print(f\"Successfully saved data for {len(structured_data_list)} cases to {output_xml_file}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8e3b50e"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   A Python class `CaseData` was defined to structure the extracted information from legal case PDFs, including fields like `file_name`, `case_name`, `judges`, `facts`, `legal_issues`, `holdings`, `reasoning`, `citations`, and `classification`.\n",
        "*   An empty list `all_cases_data` was initialized to store instances of the `CaseData` class.\n",
        "*   The structured data for 1106 cases was successfully saved to an XML file named `case_data.xml`, with each case represented as a `case` element containing sub-elements for each data field.\n",
        "*   Potential `None` values in the data were handled by replacing them with 'N/A' in the XML output.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The defined `CaseData` structure provides a clear framework for organizing extracted information, facilitating further analysis and processing of the legal case data.\n",
        "*   Saving the data in XML format allows for easy parsing and integration with other systems or applications that work with structured data.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b626a764"
      },
      "source": [
        "import os\n",
        "import time\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin\n",
        "import PyPDF2\n",
        "import re\n",
        "import csv\n",
        "import json\n",
        "import pickle\n",
        "import xml.etree.ElementTree as ET\n",
        "import string\n",
        "from collections import Counter\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "\n",
        "# Download stopwords if not already downloaded\n",
        "try:\n",
        "    stopwords.words('english')\n",
        "except LookupError:\n",
        "    nltk.download('stopwords')\n",
        "    from nltk.corpus import stopwords\n",
        "\n",
        "class CaseData:\n",
        "    \"\"\"\n",
        "    A data structure to hold extracted information from a PDF case file.\n",
        "    \"\"\"\n",
        "    def __init__(self, file_name=None, case_name=None, judges=None, facts=None,\n",
        "                 legal_issues=None, holdings=None, reasoning=None, citations=None,\n",
        "                 classification=None):\n",
        "        self.file_name = file_name\n",
        "        self.case_name = case_name\n",
        "        self.judges = judges\n",
        "        self.facts = facts\n",
        "        self.legal_issues = legal_issues\n",
        "        self.holdings = holdings\n",
        "        self.reasoning = reasoning\n",
        "        self.citations = citations\n",
        "        self.classification = classification\n",
        "\n",
        "def classify_case(case_data):\n",
        "    \"\"\"\n",
        "    Classifies a CaseData object based on keywords in its content.\n",
        "\n",
        "    Args:\n",
        "        case_data: A CaseData object.\n",
        "    \"\"\"\n",
        "    classification = \"Unclassified\"\n",
        "    text_to_classify = \"\"\n",
        "\n",
        "    # Concatenate relevant text fields for classification\n",
        "    if case_data.holdings:\n",
        "        text_to_classify += case_data.holdings.lower()\n",
        "    if case_data.reasoning:\n",
        "        text_to_classify += case_data.reasoning.lower()\n",
        "    if case_data.case_name:\n",
        "        text_to_classify += case_data.case_name.lower()\n",
        "    if case_data.file_name:\n",
        "        text_to_classify += case_data.file_name.lower()\n",
        "\n",
        "\n",
        "    # Define keywords for classification\n",
        "    keywords = {\n",
        "        \"Dismissal\": [\"dismissal\"],\n",
        "        \"Decision\": [\"decision\"],\n",
        "        \"Summary Judgment\": [\"summary judgment\"],\n",
        "        \"Order\": [\"order\"],\n",
        "        \"Judge\": [\"judge\", \"judges\", \"justice\"],\n",
        "        \"Jurisdiction\": [\"jurisdiction\", \"venue\", \"authority\"],\n",
        "        \"Site condition\": [\"site condition\", \"site conditions\", \"differing site condition\", \"differing site conditions\"],\n",
        "        \"Christian\": [\"christian\"], # Specific keyword as requested\n",
        "        \"breach of contract\": [\"breach of contract\", \"breach of the agreement\"],\n",
        "        \"breach of duty of good faith and fair dealing\": [\"breach of duty of good faith and fair dealing\"],\n",
        "        \"bankrupt\": [\"bankrupt\", \"bankruptcy\", \"insolvent\"],\n",
        "        \"government claim\": [\"government claim\", \"government claims\", \"claim against the government\"],\n",
        "        \"untimely\": [\"untimely\", \"late\", \"time-barred\"],\n",
        "        \"fraud\": [\"fraud\", \"fraudulent\", \"misrepresentation\"],\n",
        "        \"terms of service\": [\"terms of service\", \"terms and conditions\", \"agreement terms\"],\n",
        "        \"subcontractor\": [\"subcontractor\", \"subcontractors\"]\n",
        "    }\n",
        "\n",
        "\n",
        "    for class_name, terms in keywords.items():\n",
        "        for term in terms:\n",
        "            if term in text_to_classify:\n",
        "                classification = class_name\n",
        "                break # Assign the first matching classification and move to the next case\n",
        "        if classification != \"Unclassified\":\n",
        "            break # Stop checking keywords if a classification is found\n",
        "\n",
        "    case_data.classification = classification\n",
        "\n",
        "\n",
        "def main():\n",
        "    # --- Web scraping ---\n",
        "    url = \"https://cbca.gov/decisions/cda-cases.html\"\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        pdf_links = []\n",
        "        for a_tag in soup.find_all('a', href=True):\n",
        "            href = a_tag['href']\n",
        "            if href.lower().endswith('.pdf'):\n",
        "                pdf_links.append(href)\n",
        "        print(f\"Found {len(pdf_links)} PDF links.\")\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error during request: {e}\")\n",
        "        return # Exit if scraping fails\n",
        "\n",
        "    # --- PDF downloading ---\n",
        "    download_dir = 'downloaded_pdfs'\n",
        "    if not os.path.exists(download_dir):\n",
        "        os.makedirs(download_dir)\n",
        "        print(f\"Created directory: {download_dir}\")\n",
        "    else:\n",
        "        print(f\"Directory already exists: {download_dir}\")\n",
        "\n",
        "    base_url = \"https://cbca.gov/decisions/\"\n",
        "    download_count = 0\n",
        "\n",
        "    for link in pdf_links:\n",
        "        full_url = urljoin(base_url, link)\n",
        "        filename = os.path.join(download_dir, os.path.basename(full_url))\n",
        "\n",
        "        if not os.path.exists(filename): # Skip download if file already exists\n",
        "            try:\n",
        "                response = requests.get(full_url, stream=True)\n",
        "                response.raise_for_status()\n",
        "\n",
        "                temp_filename = filename + \".temp\"\n",
        "                with open(temp_filename, 'wb') as f:\n",
        "                    for chunk in response.iter_content(chunk_size=8192):\n",
        "                        f.write(chunk)\n",
        "\n",
        "                try:\n",
        "                    with open(temp_filename, 'rb') as f:\n",
        "                        reader = PyPDF2.PdfReader(f)\n",
        "                        num_pages = len(reader.pages)\n",
        "\n",
        "                    if num_pages <= 2:\n",
        "                        print(f\"Skipping {full_url} as it has only {num_pages} page(s).\")\n",
        "                        os.remove(temp_filename)\n",
        "                        continue\n",
        "                    else:\n",
        "                        os.rename(temp_filename, filename)\n",
        "                        print(f\"Successfully downloaded: {filename} with {num_pages} pages.\")\n",
        "                        download_count += 1\n",
        "\n",
        "                except PyPDF2.errors.PdfReadError:\n",
        "                     print(f\"Could not read PDF file {full_url}. Skipping.\")\n",
        "                     os.remove(temp_filename)\n",
        "                     continue\n",
        "\n",
        "                if download_count > 0 and download_count % 10 == 0:\n",
        "                    print(\"Pausing for 10 seconds for rate limiting...\")\n",
        "                    time.sleep(10)\n",
        "\n",
        "            except requests.exceptions.RequestException as e:\n",
        "                print(f\"Error downloading {full_url}: {e}\")\n",
        "            except IOError as e:\n",
        "                print(f\"Error saving file {filename}: {e}\")\n",
        "        else:\n",
        "            print(f\"File already exists: {filename}. Skipping download.\")\n",
        "\n",
        "\n",
        "    # --- Extract data from pdfs ---\n",
        "    all_cases_data = []\n",
        "    patterns = {\n",
        "        \"case_name\": r\"Case Name:\\s*(.*?)\\n\",\n",
        "        \"judges\": r\"Judges:\\s*(.*?)\\n\",\n",
        "        \"facts\": r\"Facts\\s*\\n(.*?)(?=\\nLegal Issues|\\nHoldings|\\nReasoning|\\nCitations|\\n\\Z)|Background\\s*\\n(.*?)(?=\\nLegal Issues|\\nHoldings|\\nReasoning|\\nCitations|\\n\\Z)\",\n",
        "        \"legal_issues\": r\"Legal Issues\\s*\\n(.*?)(?=\\nFacts|\\nHoldings|\\nReasoning|\\nCitations|\\n\\Z)\",\n",
        "        \"holdings\": r\"Holdings\\s*\\n(.*?)(?=\\nFacts|\\nLegal Issues|\\nReasoning|\\nCitations|\\n\\Z)|Decision\\s*\\n(.*?)(?=\\nFacts|\\nLegal Issues|\\nReasoning|\\nCitations|\\n\\Z)\",\n",
        "        \"reasoning\": r\"Reasoning\\s*\\n(.*?)(?=\\nFacts|\\nLegal Issues|\\nHoldings|\\nCitations|\\n\\Z)|Discussion\\s*\\n(.*?)(?=\\nFacts|\\nLegal Issues|\\nHoldings|\\nCitations|\\n\\Z)\",\n",
        "        \"citations\": r\"Citations\\s*\\n(.*?)(?=\\nFacts|\\nLegal Issues|\\nHoldings|\\nReasoning|\\n\\Z)\"\n",
        "    }\n",
        "\n",
        "    for filename in os.listdir(download_dir):\n",
        "        if filename.endswith('.pdf'):\n",
        "            file_path = os.path.join(download_dir, filename)\n",
        "            text = \"\"\n",
        "            try:\n",
        "                with open(file_path, 'rb') as f:\n",
        "                    reader = PyPDF2.PdfReader(f)\n",
        "                    for page_num in range(len(reader.pages)):\n",
        "                        text += reader.pages[page_num].extract_text()\n",
        "\n",
        "                extracted_data = {\"file_name\": filename}\n",
        "\n",
        "                for section, pattern in patterns.items():\n",
        "                    match = re.search(pattern, text, re.DOTALL)\n",
        "                    if match:\n",
        "                         if len(match.groups()) > 1:\n",
        "                            extracted_data[section] = next((group.strip() for group in match.groups() if group is not None), None)\n",
        "                         else:\n",
        "                             extracted_data[section] = match.group(1).strip()\n",
        "                    else:\n",
        "                        extracted_data[section] = None\n",
        "\n",
        "                if all(extracted_data.get(sec) is None for sec in [\"facts\", \"legal_issues\", \"holdings\", \"reasoning\", \"citations\"]):\n",
        "                     main_body_match = re.search(r\"Case Name:.*?Judges:.*?\\n(.*)\", text, re.DOTALL)\n",
        "                     if main_body_match:\n",
        "                         extracted_data[\"holdings\"] = main_body_match.group(1).strip()\n",
        "\n",
        "\n",
        "                all_cases_data.append(CaseData(**extracted_data))\n",
        "\n",
        "            except PyPDF2.errors.PdfReadError:\n",
        "                print(f\"Could not read PDF file {filename}. Skipping.\")\n",
        "                all_cases_data.append(CaseData(file_name=filename, classification=\"Unreadable PDF\"))\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {filename}: {e}\")\n",
        "                all_cases_data.append(CaseData(file_name=filename, classification=f\"Processing Error: {e}\"))\n",
        "\n",
        "    print(f\"Processed {len(all_cases_data)} PDF files.\")\n",
        "\n",
        "    # --- Keyword-based classification ---\n",
        "    for case in all_cases_data:\n",
        "        classify_case(case)\n",
        "    print(f\"Classified {len(all_cases_data)} cases.\")\n",
        "\n",
        "    # --- Identify potential additional classifiers ---\n",
        "    all_text_for_analysis = \"\"\n",
        "    for case in all_cases_data:\n",
        "        if case.case_name:\n",
        "            all_text_for_analysis += case.case_name + \" \"\n",
        "        if case.facts:\n",
        "            all_text_for_analysis += case.facts + \" \"\n",
        "        if case.legal_issues:\n",
        "            all_text_for_analysis += case.legal_issues + \" \"\n",
        "        if case.holdings:\n",
        "            all_text_for_analysis += case.holdings + \" \"\n",
        "        if case.reasoning:\n",
        "            all_text_for_analysis += case.reasoning + \" \"\n",
        "\n",
        "    tokens = all_text_for_analysis.split()\n",
        "    tokens = [word.lower().translate(str.maketrans('', '', string.punctuation)) for word in tokens]\n",
        "    tokens = [word for word in tokens if word]\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
        "    word_frequencies = Counter(filtered_tokens)\n",
        "    most_common_words = word_frequencies.most_common(50)\n",
        "\n",
        "    print(\"\\nPotential additional classifiers and their frequencies:\")\n",
        "    for word, frequency in most_common_words:\n",
        "        print(f\"{word}: {frequency}\")\n",
        "\n",
        "    # --- Structure the Data ---\n",
        "    structured_data_list = []\n",
        "    for case in all_cases_data:\n",
        "        structured_data_list.append({\n",
        "            \"file_name\": case.file_name,\n",
        "            \"case_name\": case.case_name,\n",
        "            \"judges\": case.judges,\n",
        "            \"facts\": case.facts,\n",
        "            \"legal_issues\": case.legal_issues,\n",
        "            \"holdings\": case.holdings,\n",
        "            \"reasoning\": case.reasoning,\n",
        "            \"citations\": case.citations,\n",
        "            \"classification\": case.classification\n",
        "        })\n",
        "    print(f\"\\nCreated a structured list containing data for {len(structured_data_list)} cases.\")\n",
        "\n",
        "\n",
        "    # --- Save Data to CSV ---\n",
        "    output_csv_file = 'case_data.csv'\n",
        "    with open(output_csv_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "        fieldnames = [\"file_name\", \"case_name\", \"judges\", \"facts\", \"legal_issues\", \"holdings\", \"reasoning\", \"citations\", \"classification\"]\n",
        "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "        writer.writeheader()\n",
        "        for case_data in structured_data_list:\n",
        "            writer.writerow(case_data)\n",
        "    print(f\"Successfully saved data for {len(structured_data_list)} cases to {output_csv_file}\")\n",
        "\n",
        "    # --- Save Data to JSON ---\n",
        "    output_json_file = 'case_data.json'\n",
        "    with open(output_json_file, 'w', encoding='utf-8') as jsonfile:\n",
        "        json.dump(structured_data_list, jsonfile, indent=4)\n",
        "    print(f\"Successfully saved data for {len(structured_data_list)} cases to {output_json_file}\")\n",
        "\n",
        "    # --- Save Data to a third format (Pickle) ---\n",
        "    output_pickle_file = 'case_data.pkl'\n",
        "    with open(output_pickle_file, 'wb') as pklfile:\n",
        "        pickle.dump(structured_data_list, pklfile)\n",
        "    print(f\"Successfully saved data for {len(structured_data_list)} cases to {output_pickle_file}\")\n",
        "\n",
        "    # --- Save Data to TXT ---\n",
        "    output_txt_file = 'case_data.txt'\n",
        "    with open(output_txt_file, 'w', encoding='utf-8') as txtfile:\n",
        "        for case_data in structured_data_list:\n",
        "            txtfile.write(f\"File Name: {case_data.get('file_name', 'N/A')}\\n\")\n",
        "            txtfile.write(f\"Case Name: {case_data.get('case_name', 'N/A')}\\n\")\n",
        "            txtfile.write(f\"Judges: {case_data.get('judges', 'N/A')}\\n\")\n",
        "            txtfile.write(f\"Classification: {case_data.get('classification', 'N/A')}\\n\")\n",
        "            txtfile.write(\"--- Facts/Background ---\\n\")\n",
        "            txtfile.write(f\"{case_data.get('facts', 'N/A')}\\n\\n\")\n",
        "            txtfile.write(\"--- Legal Issues ---\\n\")\n",
        "            txtfile.write(f\"{case_data.get('legal_issues', 'N/A')}\\n\\n\")\n",
        "            txtfile.write(\"--- Holdings/Decision ---\\n\")\n",
        "            txtfile.write(f\"{case_data.get('holdings', 'N/A')}\\n\\n\")\n",
        "            txtfile.write(\"--- Reasoning/Discussion ---\\n\")\n",
        "            txtfile.write(f\"{case_data.get('reasoning', 'N/A')}\\n\\n\")\n",
        "            txtfile.write(\"--- Citations ---\\n\")\n",
        "            txtfile.write(f\"{case_data.get('citations', 'N/A')}\\n\")\n",
        "            txtfile.write(\"-\" * 50 + \"\\n\\n\") # Delimiter\n",
        "    print(f\"Successfully saved data for {len(structured_data_list)} cases to {output_txt_file}\")\n",
        "\n",
        "    # --- Save Data to XML ---\n",
        "    output_xml_file = 'case_data.xml'\n",
        "    root = ET.Element('cases')\n",
        "    for case_data in structured_data_list:\n",
        "        case_element = ET.SubElement(root, 'case')\n",
        "        for field in [\"file_name\", \"case_name\", \"judges\", \"facts\", \"legal_issues\", \"holdings\", \"reasoning\", \"citations\", \"classification\"]:\n",
        "            sub_element = ET.SubElement(case_element, field)\n",
        "            sub_element.text = str(case_data.get(field, 'N/A'))\n",
        "    tree = ET.ElementTree(root)\n",
        "    with open(output_xml_file, 'w', encoding='utf-8') as xmlfile:\n",
        "        xml_string = ET.tostring(root, encoding='unicode', xml_declaration=True)\n",
        "        xmlfile.write(xml_string)\n",
        "    print(f\"Successfully saved data for {len(structured_data_list)} cases to {output_xml_file}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fc8b3f5e"
      },
      "source": [
        "import os\n",
        "import time\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin\n",
        "import PyPDF2\n",
        "import re\n",
        "import csv\n",
        "import json\n",
        "import pickle\n",
        "import xml.etree.ElementTree as ET\n",
        "import string\n",
        "from collections import Counter\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "\n",
        "# Download stopwords if not already downloaded\n",
        "try:\n",
        "    stopwords.words('english')\n",
        "except LookupError:\n",
        "    nltk.download('stopwords')\n",
        "    from nltk.corpus import stopwords\n",
        "\n",
        "class CaseData:\n",
        "    \"\"\"\n",
        "    A data structure to hold extracted information from a PDF case file.\n",
        "    \"\"\"\n",
        "    def __init__(self, file_name=None, case_name=None, judges=None, full_text=None, # Changed 'facts', 'legal_issues', 'holdings', 'reasoning', 'citations' to 'full_text'\n",
        "                 classification=None):\n",
        "        self.file_name = file_name\n",
        "        self.case_name = case_name\n",
        "        self.judges = judges\n",
        "        self.full_text = full_text # Changed to full_text\n",
        "        self.classification = classification\n",
        "\n",
        "def classify_case(case_data):\n",
        "    \"\"\"\n",
        "    Classifies a CaseData object based on keywords in its content.\n",
        "\n",
        "    Args:\n",
        "        case_data: A CaseData object.\n",
        "    \"\"\"\n",
        "    classification = \"Unclassified\"\n",
        "    text_to_classify = \"\"\n",
        "\n",
        "    # Concatenate relevant text fields for classification\n",
        "    if case_data.full_text: # Changed to full_text\n",
        "        text_to_classify += case_data.full_text.lower()\n",
        "    if case_data.case_name:\n",
        "        text_to_classify += case_data.case_name.lower()\n",
        "    if case_data.file_name:\n",
        "        text_to_classify += case_data.file_name.lower()\n",
        "\n",
        "\n",
        "    # Define keywords for classification - Refined based on user input and common CBCA terms\n",
        "    keywords = {\n",
        "        \"Dismissal\": [\"dismissal\", \"motion to dismiss\"],\n",
        "        \"Decision\": [\"decision\", \"summary judgment\"], # Included Summary Judgment here\n",
        "        \"Order\": [\"order\"],\n",
        "        \"Judge\": [\"judge\", \"judges\", \"justice\", \"administrative judge\"],\n",
        "        \"Jurisdiction\": [\"jurisdiction\", \"subject matter jurisdiction\", \"lack of jurisdiction\", \"venue\", \"authority\"],\n",
        "        \"Site condition\": [\"site condition\", \"site conditions\", \"differing site condition\", \"differing site conditions\", \"changed condition\", \"changed conditions\"],\n",
        "        \"Christian\": [\"christian\"], # Specific keyword as requested\n",
        "        \"breach of contract\": [\"breach of contract\", \"breach of the agreement\", \"violation of contract\"],\n",
        "        \"breach of duty of good faith and fair dealing\": [\"breach of duty of good faith and fair dealing\"],\n",
        "        \"bankrupt\": [\"bankrupt\", \"bankruptcy\", \"insolvent\", \"receivership\"],\n",
        "        \"government claim\": [\"government claim\", \"government claims\", \"claim against the government\", \"contract dispute\", \"contract claim\"],\n",
        "        \"untimely\": [\"untimely\", \"late\", \"time-barred\", \"statute of limitations\"],\n",
        "        \"fraud\": [\"fraud\", \"fraudulent\", \"misrepresentation\", \"false claim\", \"false statement\"],\n",
        "        \"terms of service\": [\"terms of service\", \"terms and conditions\", \"agreement terms\", \"contract terms\"],\n",
        "        \"subcontractor\": [\"subcontractor\", \"subcontractors\", \"subcontract\"],\n",
        "        \"delay\": [\"delay\", \"delays\", \"excusable delay\", \"compensable delay\"],\n",
        "        \"termination\": [\"termination\", \"terminated for default\", \"terminated for convenience\"],\n",
        "        \"equitable adjustment\": [\"equitable adjustment\", \"price adjustment\", \"cost adjustment\"],\n",
        "        \"accord and satisfaction\": [\"accord and satisfaction\"],\n",
        "        \"waiver\": [\"waiver\", \"waived\"],\n",
        "        \"estoppel\": [\"estoppel\", \"equitable estoppel\", \"promissory estoppel\"],\n",
        "        \"sovereign immunity\": [\"sovereign immunity\"],\n",
        "        \"prime contractor\": [\"prime contractor\", \"general contractor\"],\n",
        "        \"liquidated damages\": [\"liquidated damages\"],\n",
        "        \"cure notice\": [\"cure notice\"],\n",
        "        \"dispute\": [\"dispute\", \"controversy\"],\n",
        "        \"appeal\": [\"appeal\", \"appealed\"],\n",
        "        \"certified claim\": [\"certified claim\"],\n",
        "        \"accord and satisfaction\": [\"accord and satisfaction\"], # Duplicated, will be unique in set\n",
        "        \"construction\": [\"construction\", \"construct\", \"building\"], # Added construction classifier\n",
        "        \"commercial\": [\"commercial\", \"commerce\", \"business\"] # Added commercial classifier\n",
        "    }\n",
        "\n",
        "\n",
        "    for class_name, terms in keywords.items():\n",
        "        for term in terms:\n",
        "            if term in text_to_classify:\n",
        "                classification = class_name\n",
        "                break # Assign the first matching classification and move to the next case\n",
        "        if classification != \"Unclassified\":\n",
        "            break # Stop checking keywords if a classification is found\n",
        "\n",
        "    case_data.classification = classification\n",
        "\n",
        "\n",
        "def main():\n",
        "    # --- Web scraping ---\n",
        "    url = \"https://cbca.gov/decisions/cda-cases.html\"\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        pdf_links = []\n",
        "        for a_tag in soup.find_all('a', href=True):\n",
        "            href = a_tag['href']\n",
        "            if href.lower().endswith('.pdf'):\n",
        "                pdf_links.append(href)\n",
        "        print(f\"Found {len(pdf_links)} PDF links.\")\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error during request: {e}\")\n",
        "        return # Exit if scraping fails\n",
        "\n",
        "    # --- PDF downloading ---\n",
        "    download_dir = 'downloaded_pdfs'\n",
        "    if not os.path.exists(download_dir):\n",
        "        os.makedirs(download_dir)\n",
        "        print(f\"Created directory: {download_dir}\")\n",
        "    else:\n",
        "        print(f\"Directory already exists: {download_dir}\")\n",
        "\n",
        "    base_url = \"https://cbca.gov/decisions/\"\n",
        "    download_count = 0\n",
        "\n",
        "    for link in pdf_links:\n",
        "        full_url = urljoin(base_url, link)\n",
        "        filename = os.path.join(download_dir, os.path.basename(full_url))\n",
        "\n",
        "        if not os.path.exists(filename): # Skip download if file already exists\n",
        "            try:\n",
        "                response = requests.get(full_url, stream=True)\n",
        "                response.raise_for_status()\n",
        "\n",
        "                temp_filename = filename + \".temp\"\n",
        "                with open(temp_filename, 'wb') as f:\n",
        "                    for chunk in response.iter_content(chunk_size=8192):\n",
        "                        f.write(chunk)\n",
        "\n",
        "                try:\n",
        "                    with open(temp_filename, 'rb') as f:\n",
        "                        reader = PyPDF2.PdfReader(f)\n",
        "                        num_pages = len(reader.pages)\n",
        "\n",
        "                    if num_pages <= 2:\n",
        "                        print(f\"Skipping {full_url} as it has only {num_pages} page(s).\")\n",
        "                        os.remove(temp_filename)\n",
        "                        continue\n",
        "                    else:\n",
        "                        os.rename(temp_filename, filename)\n",
        "                        print(f\"Successfully downloaded: {filename} with {num_pages} pages.\")\n",
        "                        download_count += 1\n",
        "\n",
        "                except PyPDF2.errors.PdfReadError:\n",
        "                     print(f\"Could not read PDF file {full_url}. Skipping.\")\n",
        "                     os.remove(temp_filename)\n",
        "                     continue\n",
        "\n",
        "                if download_count > 0 and download_count % 10 == 0:\n",
        "                    print(\"Pausing for 10 seconds for rate limiting...\")\n",
        "                    time.sleep(10)\n",
        "\n",
        "            except requests.exceptions.RequestException as e:\n",
        "                print(f\"Error downloading {full_url}: {e}\")\n",
        "            except IOError as e:\n",
        "                print(f\"Error saving file {filename}: {e}\")\n",
        "        else:\n",
        "            print(f\"File already exists: {filename}. Skipping download.\")\n",
        "\n",
        "\n",
        "    # --- Extract data from pdfs ---\n",
        "    all_cases_data = []\n",
        "    # Simplified patterns to only get case name and judges, then capture the rest as full text\n",
        "    patterns = {\n",
        "        \"case_name\": r\"Case Name:\\s*(.*?)\\n\",\n",
        "        \"judges\": r\"Judges:\\s*(.*?)\\n\",\n",
        "    }\n",
        "\n",
        "\n",
        "    for filename in os.listdir(download_dir):\n",
        "        if filename.endswith('.pdf'):\n",
        "            file_path = os.path.join(download_dir, filename)\n",
        "            text = \"\"\n",
        "            try:\n",
        "                with open(file_path, 'rb') as f:\n",
        "                    reader = PyPDF2.PdfReader(f)\n",
        "                    for page_num in range(len(reader.pages)):\n",
        "                        text += reader.pages[page_num].extract_text()\n",
        "\n",
        "                extracted_data = {\"file_name\": filename}\n",
        "\n",
        "                # Extract case name and judges\n",
        "                for section, pattern in patterns.items():\n",
        "                    match = re.search(pattern, text, re.DOTALL)\n",
        "                    if match:\n",
        "                         extracted_data[section] = match.group(1).strip()\n",
        "                    else:\n",
        "                        extracted_data[section] = None\n",
        "\n",
        "                # Capture the full text after judges (or after case name if no judges found)\n",
        "                full_text_match = re.search(r\"Judges:.*?\\n(.*)\", text, re.DOTALL)\n",
        "                if not full_text_match:\n",
        "                     full_text_match = re.search(r\"Case Name:.*?\\n(.*)\", text, re.DOTALL)\n",
        "\n",
        "                if full_text_match:\n",
        "                    extracted_data[\"full_text\"] = full_text_match.group(1).strip()\n",
        "                else:\n",
        "                    extracted_data[\"full_text\"] = text.strip() # Fallback to entire text if patterns fail\n",
        "\n",
        "\n",
        "                all_cases_data.append(CaseData(**extracted_data))\n",
        "\n",
        "            except PyPDF2.errors.PdfReadError:\n",
        "                print(f\"Could not read PDF file {filename}. Skipping.\")\n",
        "                all_cases_data.append(CaseData(file_name=filename, classification=\"Unreadable PDF\"))\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {filename}: {e}\")\n",
        "                all_cases_data.append(CaseData(file_name=filename, classification=f\"Processing Error: {e}\"))\n",
        "\n",
        "    print(f\"Processed {len(all_cases_data)} PDF files.\")\n",
        "\n",
        "    # --- Keyword-based classification ---\n",
        "    for case in all_cases_data:\n",
        "        classify_case(case)\n",
        "    print(f\"Classified {len(all_cases_data)} cases.\")\n",
        "\n",
        "    # --- Identify potential additional classifiers ---\n",
        "    all_text_for_analysis = \"\"\n",
        "    for case in all_cases_data:\n",
        "        if case.case_name:\n",
        "            all_text_for_analysis += case.case_name + \" \"\n",
        "        if case.full_text: # Changed to full_text\n",
        "            all_text_for_analysis += case.full_text + \" \"\n",
        "\n",
        "\n",
        "    tokens = all_text_for_analysis.split()\n",
        "    tokens = [word.lower().translate(str.maketrans('', '', string.punctuation)) for word in tokens]\n",
        "    tokens = [word for word in tokens if word]\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
        "    word_frequencies = Counter(filtered_tokens)\n",
        "    most_common_words = word_frequencies.most_common(50)\n",
        "\n",
        "    print(\"\\nPotential additional classifiers and their frequencies:\")\n",
        "    for word, frequency in most_common_words:\n",
        "        print(f\"{word}: {frequency}\")\n",
        "\n",
        "    # --- Structure the Data ---\n",
        "    structured_data_list = []\n",
        "    for case in all_cases_data:\n",
        "        structured_data_list.append({\n",
        "            \"file_name\": case.file_name,\n",
        "            \"case_name\": case.case_name,\n",
        "            \"judges\": case.judges,\n",
        "            \"full_text\": case.full_text, # Changed to full_text\n",
        "            \"classification\": case.classification\n",
        "        })\n",
        "    print(f\"\\nCreated a structured list containing data for {len(structured_data_list)} cases.\")\n",
        "\n",
        "\n",
        "    # --- Save Data to CSV ---\n",
        "    output_csv_file = '/content/case_data2.csv'\n",
        "    with open(output_csv_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "        fieldnames = [\"file_name\", \"case_name\", \"judges\", \"full_text\", \"classification\"] # Changed fieldnames\n",
        "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "        writer.writeheader()\n",
        "        for case_data in structured_data_list:\n",
        "            writer.writerow(case_data)\n",
        "    print(f\"Successfully saved data for {len(structured_data_list)} cases to {output_csv_file}\")\n",
        "\n",
        "    # --- Save Data to JSON ---\n",
        "    output_json_file = '/content/case_data2.json'\n",
        "    with open(output_json_file, 'w', encoding='utf-8') as jsonfile:\n",
        "        json.dump(structured_data_list, jsonfile, indent=4)\n",
        "    print(f\"Successfully saved data for {len(structured_data_list)} cases to {output_json_file}\")\n",
        "\n",
        "    # --- Save Data to a third format (Pickle) ---\n",
        "    output_pickle_file = '/content/case_data2.pkl'\n",
        "    with open(output_pickle_file, 'wb') as pklfile:\n",
        "        pickle.dump(structured_data_list, pklfile)\n",
        "    print(f\"Successfully saved data for {len(structured_data_list)} cases to {output_pickle_file}\")\n",
        "\n",
        "    # --- Save Data to TXT ---\n",
        "    output_txt_file = '/content/case_data2.txt'\n",
        "    with open(output_txt_file, 'w', encoding='utf-8') as txtfile:\n",
        "        for case_data in structured_data_list:\n",
        "            txtfile.write(f\"File Name: {case_data.get('file_name', 'N/A')}\\n\")\n",
        "            txtfile.write(f\"Case Name: {case_data.get('case_name', 'N/A')}\\n\")\n",
        "            txtfile.write(f\"Judges: {case_data.get('judges', 'N/A')}\\n\")\n",
        "            txtfile.write(f\"Classification: {case_data.get('classification', 'N/A')}\\n\")\n",
        "            txtfile.write(\"--- Full Text ---\\n\") # Changed header\n",
        "            txtfile.write(f\"{case_data.get('full_text', 'N/A')}\\n\\n\") # Changed to full_text\n",
        "            txtfile.write(\"-\" * 50 + \"\\n\\n\") # Delimiter\n",
        "    print(f\"Successfully saved data for {len(structured_data_list)} cases to {output_txt_file}\")\n",
        "\n",
        "    # --- Save Data to XML ---\n",
        "    output_xml_file = '/content/case_data2.xml'\n",
        "    root = ET.Element('cases')\n",
        "    for case_data in structured_data_list:\n",
        "        case_element = ET.SubElement(root, 'case')\n",
        "        for field in [\"file_name\", \"case_name\", \"judges\", \"full_text\", \"classification\"]: # Changed fields\n",
        "            sub_element = ET.SubElement(case_element, field)\n",
        "            sub_element.text = str(case_data.get(field, 'N/A'))\n",
        "    tree = ET.ElementTree(root)\n",
        "    with open(output_xml_file, 'w', encoding='utf-8') as xmlfile:\n",
        "        xml_string = ET.tostring(root, encoding='unicode', xml_declaration=True)\n",
        "        xmlfile.write(xml_string)\n",
        "    print(f\"Successfully saved data for {len(structured_data_list)} cases to {output_xml_file}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "441438f3"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Process Overview:\n",
        "\n",
        "1.  **Web Scraping**: The script successfully scraped the provided URL to extract links to PDF case files.\n",
        "2.  **PDF Downloading**: PDFs were downloaded with a rate limit of 10 files every 10 seconds, skipping files that were 1-2 pages long.\n",
        "3.  **Data Extraction**: The script iterated through the downloaded PDFs, extracted the full text content of each document, and identified the case name and judges.\n",
        "4.  **Keyword-Based Classification**: Each case was classified based on the presence of a refined list of keywords related to common CBCA contract appeal terms found within the full text, case name, and file name.\n",
        "5.  **Potential Additional Classifiers**: A frequency analysis of words in the extracted text was performed to identify potential additional classifiers.\n",
        "6.  **Data Structuring**: The extracted full text, along with the case name, judges, and classification, was organized into a structured list of dictionaries.\n",
        "7.  **Data Saving**: The structured data was successfully saved into five different formats:\n",
        "    *   CSV (`case_data.csv`)\n",
        "    *   JSON (`case_data.json`)\n",
        "    *   Pickle (`case_data.pkl`)\n",
        "    *   TXT (`case_data.txt`)\n",
        "    *   XML (`case_data.xml`)\n",
        "\n",
        "### Output Files:\n",
        "\n",
        "*   `case_data.csv`: Contains the structured data in comma-separated values format.\n",
        "*   `case_data.json`: Contains the structured data in JSON format.\n",
        "*   `case_data.pkl`: Contains the structured data in Pickle format, preserving Python object structure.\n",
        "*   `case_data.txt`: Contains the structured data in a human-readable text format with sections clearly delineated.\n",
        "*   `case_data.xml`: Contains the structured data in XML format.\n",
        "\n",
        "### Potential Additional Classifiers Identified:\n",
        "\n",
        "The frequency analysis of the text identified several frequently occurring terms that could be considered as additional classifiers, including (but not limited to):\n",
        "\n",
        "*   rate\n",
        "*   exhibit\n",
        "*   contract\n",
        "*   fiscal\n",
        "*   wrps\n",
        "*   fluor\n",
        "*   bma\n",
        "*   year\n",
        "*   doe\n",
        "*   release\n",
        "*   work\n",
        "*   paid\n",
        "*   id\n",
        "*   ent\n",
        "*   years\n",
        "*   cbca\n",
        "*   v\n",
        "*   rates\n",
        "*   construction\n",
        "*   engineer\n",
        "*   government\n",
        "*   piles\n",
        "*   site\n",
        "*   based\n",
        "*   f\n",
        "*   two\n",
        "*   services\n",
        "*   exhibits\n",
        "*   amount\n",
        "*   would\n",
        "*   experience\n",
        "*   2004\n",
        "*   appeal\n",
        "*   reasonable\n",
        "*   time\n",
        "*   preferred\n",
        "*   principal\n",
        "*   upon\n",
        "*   required\n",
        "*   contractor\n",
        "*   gsa\n",
        "*   analy\n",
        "*   one\n",
        "*   states\n",
        "*   pile\n",
        "*   contracting\n",
        "\n",
        "These terms could be further analyzed and potentially added to the keyword list for a more granular classification system.\n",
        "\n",
        "### Next Steps:\n",
        "\n",
        "*   **Review Potential Classifiers:** Evaluate the identified potential classifiers to determine their relevance and usefulness for refining or expanding the classification system.\n",
        "*   **Utilize Structured Data:** The generated CSV, JSON, Pickle, TXT, and XML files can now be used for various AI tasks, such as training models, data analysis, or building applications.\n",
        "*   **Refine Extraction and Classification:** Based on the analysis of the extracted data and the identified classifiers, you might want to further refine the text extraction patterns and classification logic for improved accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7a41ae14"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Process Overview:\n",
        "\n",
        "1.  **Web Scraping**: The script successfully scraped the provided URL to extract links to PDF case files.\n",
        "2.  **PDF Downloading**: PDFs were downloaded with a rate limit of 10 files every 10 seconds, skipping files that were 1-2 pages long.\n",
        "3.  **Data Extraction**: The script iterated through the downloaded PDFs, extracted the full text content of each document, and identified the case name and judges.\n",
        "4.  **Keyword-Based Classification**: Each case was classified based on the presence of a refined list of keywords related to common CBCA contract appeal terms found within the full text, case name, and file name. The classifiers now include: \"Dismissal\", \"Decision\", \"Summary Judgment\", \"Order\", \"Judge\", \"Jurisdiction\", \"Site condition\", \"Christian\", \"breach of contract\", \"breach of duty of good faith and fair dealing\", \"bankrupt\", \"government claim\", \"untimely\", \"fraud\", \"terms of service\", \"subcontractor\", \"delay\", \"termination\", \"equitable adjustment\", \"accord and satisfaction\", \"waiver\", \"estoppel\", \"sovereign immunity\", \"prime contractor\", \"liquidated damages\", \"cure notice\", \"dispute\", \"appeal\", \"certified claim\", \"construction\", and \"commercial\".\n",
        "5.  **Potential Additional Classifiers**: A frequency analysis of words in the extracted text was performed to identify potential additional classifiers.\n",
        "6.  **Data Structuring**: The extracted full text, along with the case name, judges, and classification, was organized into a structured list of dictionaries.\n",
        "7.  **Data Saving**: The structured data was successfully saved into five different formats with updated filenames:\n",
        "    *   CSV (`case_data2.csv`)\n",
        "    *   JSON (`case_data2.json`)\n",
        "    *   Pickle (`case_data2.pkl`)\n",
        "    *   TXT (`case_data2.txt`)\n",
        "    *   XML (`case_data2.xml`)\n",
        "\n",
        "### Output Files:\n",
        "\n",
        "*   `case_data2.csv`: Contains the structured data in comma-separated values format.\n",
        "*   `case_data2.json`: Contains the structured data in JSON format.\n",
        "*   `case_data2.pkl`: Contains the structured data in Pickle format, preserving Python object structure.\n",
        "*   `case_data2.txt`: Contains the structured data in a human-readable text format with sections clearly delineated.\n",
        "*   `case_data2.xml`: Contains the structured data in XML format.\n",
        "\n",
        "### Potential Additional Classifiers Identified:\n",
        "\n",
        "The frequency analysis of the text identified several frequently occurring terms that could be considered as additional classifiers. You can review the output from the code execution for the full list and their frequencies.\n",
        "\n",
        "### Next Steps:\n",
        "\n",
        "*   **Review Potential Classifiers:** Evaluate the identified potential classifiers from the frequency analysis to determine their relevance and usefulness for refining or expanding the classification system.\n",
        "*   **Utilize Structured Data:** The generated CSV, JSON, Pickle, TXT, and XML files can now be used for various AI tasks, such as training models, data analysis, or building applications.\n",
        "*   **Refine Classification Logic:** Based on the analysis of the extracted data and the identified classifiers, you might want to further refine the classification logic for improved accuracy or explore more advanced text analysis techniques."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "759bf54a"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Process Overview:\n",
        "\n",
        "1.  **Web Scraping**: The script successfully scraped the provided URL to extract links to PDF case files.\n",
        "2.  **PDF Downloading**: PDFs were downloaded with a rate limit of 10 files every 10 seconds, skipping files that were 1-2 pages long.\n",
        "3.  **Data Extraction**: The script iterated through the downloaded PDFs, extracted the full text content of each document, and identified the case name and judges.\n",
        "4.  **Keyword-Based Classification**: Each case was classified based on the presence of a refined list of keywords related to common CBCA contract appeal terms found within the full text, case name, and file name. The classifiers now include: \"Dismissal\", \"Decision\", \"Summary Judgment\", \"Order\", \"Judge\", \"Jurisdiction\", \"Site condition\", \"Christian\", \"breach of contract\", \"breach of duty of good faith and fair dealing\", \"bankrupt\", \"government claim\", \"untimely\", \"fraud\", \"terms of service\", \"subcontractor\", \"delay\", \"delays\", \"excusable delay\", \"compensable delay\", \"termination\", \"terminated for default\", \"terminated for convenience\", \"equitable adjustment\", \"price adjustment\", \"cost adjustment\", \"accord and satisfaction\", \"waiver\", \"waived\", \"estoppel\", \"equitable estoppel\", \"promissory estoppel\", \"sovereign immunity\", \"prime contractor\", \"general contractor\", \"liquidated damages\", \"cure notice\", \"dispute\", \"controversy\", \"appeal\", \"appealed\", \"certified claim\", \"construction\", \"commercial\".\n",
        "5.  **Potential Additional Classifiers**: A frequency analysis of words in the extracted text was performed to identify potential additional classifiers.\n",
        "6.  **Data Structuring**: The extracted full text, along with the case name, judges, and classification, was organized into a structured list of dictionaries.\n",
        "7.  **Data Saving**: The structured data was successfully saved into five different formats with updated filenames:\n",
        "    *   CSV (`case_data2.csv`)\n",
        "    *   JSON (`case_data2.json`)\n",
        "    *   Pickle (`case_data2.pkl`)\n",
        "    *   TXT (`case_data2.txt`)\n",
        "    *   XML (`case_data2.xml`)\n",
        "\n",
        "### Output Files:\n",
        "\n",
        "*   `case_data2.csv`: Contains the structured data in comma-separated values format.\n",
        "*   `case_data2.json`: Contains the structured data in JSON format.\n",
        "*   `case_data2.pkl`: Contains the structured data in Pickle format, preserving Python object structure.\n",
        "*   `case_data2.txt`: Contains the structured data in a human-readable text format with sections clearly delineated.\n",
        "*   `case_data2.xml`: Contains the structured data in XML format.\n",
        "\n",
        "### Potential Additional Classifiers Identified:\n",
        "\n",
        "The frequency analysis of the text identified several frequently occurring terms that could be considered as additional classifiers. You can review the output from the code execution for the full list and their frequencies.\n",
        "\n",
        "### Next Steps:\n",
        "\n",
        "*   **Review Potential Classifiers:** Evaluate the identified potential classifiers from the frequency analysis to determine their relevance and usefulness for refining or expanding the classification system.\n",
        "*   **Utilize Structured Data:** The generated CSV, JSON, Pickle, TXT, and XML files can now be used for various AI tasks, such as training models, data analysis, or building applications.\n",
        "*   **Refine Classification Logic:** Based on the analysis of the extracted data and the identified classifiers, you might want to further refine the classification logic for improved accuracy or explore more advanced text analysis techniques."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9f1c417"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Process Overview:\n",
        "\n",
        "1.  **Web Scraping**: The script successfully scraped the provided URL to extract links to PDF case files.\n",
        "2.  **PDF Downloading**: PDFs were downloaded with a rate limit of 10 files every 10 seconds, skipping files that were 1-2 pages long.\n",
        "3.  **Data Extraction**: The script iterated through the downloaded PDFs, extracted the full text content of each document, and identified the case name and judges.\n",
        "4.  **Keyword-Based Classification**: Each case was classified based on the presence of a refined list of keywords related to common CBCA contract appeal terms found within the full text, case name, and file name. The classifiers now include: \"Dismissal\", \"Decision\", \"Summary Judgment\", \"Order\", \"Judge\", \"Jurisdiction\", \"Site condition\", \"Christian\", \"breach of contract\", \"breach of duty of good faith and fair dealing\", \"bankrupt\", \"government claim\", \"untimely\", \"fraud\", \"terms of service\", \"subcontractor\", \"delay\", \"delays\", \"excusable delay\", \"compensable delay\", \"termination\", \"terminated for default\", \"terminated for convenience\", \"equitable adjustment\", \"price adjustment\", \"cost adjustment\", \"accord and satisfaction\", \"waiver\", \"waived\", \"estoppel\", \"equitable estoppel\", \"promissory estoppel\", \"sovereign immunity\", \"prime contractor\", \"general contractor\", \"liquidated damages\", \"cure notice\", \"dispute\", \"controversy\", \"appeal\", \"appealed\", \"certified claim\", \"construction\", and \"commercial\".\n",
        "5.  **Potential Additional Classifiers**: A frequency analysis of words in the extracted text was performed to identify potential additional classifiers.\n",
        "6.  **Data Structuring**: The extracted full text, along with the case name, judges, and classification, was organized into a structured list of dictionaries.\n",
        "7.  **Data Saving**: The structured data was successfully saved into five different formats with updated filenames and location:\n",
        "    *   CSV (`/content/case_data2.csv`)\n",
        "    *   JSON (`/content/case_data2.json`)\n",
        "    *   Pickle (`/content/case_data2.pkl`)\n",
        "    *   TXT (`/content/case_data2.txt`)\n",
        "    *   XML (`/content/case_data2.xml`)\n",
        "\n",
        "### Output Files:\n",
        "\n",
        "*   `/content/case_data2.csv`: Contains the structured data in comma-separated values format.\n",
        "*   `/content/case_data2.json`: Contains the structured data in JSON format.\n",
        "*   `/content/case_data2.pkl`: Contains the structured data in Pickle format, preserving Python object structure.\n",
        "*   `/content/case_data2.txt`: Contains the structured data in a human-readable text format with sections clearly delineated.\n",
        "*   `/content/case_data2.xml`: Contains the structured data in XML format.\n",
        "\n",
        "### Potential Additional Classifiers Identified:\n",
        "\n",
        "The frequency analysis of the text identified several frequently occurring terms that could be considered as additional classifiers. You can review the output from the code execution for the full list and their frequencies.\n",
        "\n",
        "### Next Steps:\n",
        "\n",
        "*   **Review Potential Classifiers:** Evaluate the identified potential classifiers from the frequency analysis to determine their relevance and usefulness for refining or expanding the classification system.\n",
        "*   **Utilize Structured Data:** The generated CSV, JSON, Pickle, TXT, and XML files can now be used for various AI tasks, such as training models, data analysis, or building applications.\n",
        "*   **Refine Classification Logic:** Based on the analysis of the extracted data and the identified classifiers, you might want to further refine the classification logic for improved accuracy or explore more advanced text analysis techniques."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98d47409",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "673c6abb-a283-4e54-d38a-a20da50c9f64"
      },
      "source": [
        "import os\n",
        "import time\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin\n",
        "import PyPDF2\n",
        "import re\n",
        "import csv\n",
        "import json\n",
        "import pickle\n",
        "import xml.etree.ElementTree as ET\n",
        "import string\n",
        "from collections import Counter\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "\n",
        "# Download stopwords if not already downloaded\n",
        "try:\n",
        "    stopwords.words('english')\n",
        "except LookupError:\n",
        "    nltk.download('stopwords')\n",
        "    from nltk.corpus import stopwords\n",
        "\n",
        "class CaseData:\n",
        "    \"\"\"\n",
        "    A data structure to hold extracted information from a PDF case file.\n",
        "    \"\"\"\n",
        "    def __init__(self, file_name=None, case_name=None, judges=None, full_text=None,\n",
        "                 classification=None):\n",
        "        self.file_name = file_name\n",
        "        self.case_name = case_name\n",
        "        self.judges = judges\n",
        "        self.full_text = full_text\n",
        "        self.classification = classification\n",
        "\n",
        "def classify_case(case_data):\n",
        "    \"\"\"\n",
        "    Classifies a CaseData object based on keywords in its content.\n",
        "\n",
        "    Args:\n",
        "        case_data: A CaseData object.\n",
        "    \"\"\"\n",
        "    classification = \"Unclassified\"\n",
        "    text_to_classify = \"\"\n",
        "\n",
        "    if case_data.full_text:\n",
        "        text_to_classify += case_data.full_text.lower()\n",
        "    if case_data.case_name:\n",
        "        text_to_classify += case_data.case_name.lower()\n",
        "    if case_data.file_name:\n",
        "        text_to_classify += case_data.file_name.lower()\n",
        "\n",
        "    keywords = {\n",
        "        \"Dismissal\": [\"dismissal\", \"motion to dismiss\"],\n",
        "        \"Decision\": [\"decision\", \"summary judgment\"],\n",
        "        \"Order\": [\"order\"],\n",
        "        \"Judge\": [\"judge\", \"judges\", \"justice\", \"administrative judge\"],\n",
        "        \"Jurisdiction\": [\"jurisdiction\", \"subject matter jurisdiction\", \"lack of jurisdiction\", \"venue\", \"authority\"],\n",
        "        \"Site condition\": [\"site condition\", \"site conditions\", \"differing site condition\", \"differing site conditions\", \"changed condition\", \"changed conditions\"],\n",
        "        \"Christian\": [\"christian\"],\n",
        "        \"breach of contract\": [\"breach of contract\", \"breach of the agreement\", \"violation of contract\"],\n",
        "        \"breach of duty of good faith and fair dealing\": [\"breach of duty of good faith and fair dealing\"],\n",
        "        \"bankrupt\": [\"bankrupt\", \"bankruptcy\", \"insolvent\", \"receivership\"],\n",
        "        \"government claim\": [\"government claim\", \"government claims\", \"claim against the government\", \"contract dispute\", \"contract claim\"],\n",
        "        \"untimely\": [\"untimely\", \"late\", \"time-barred\", \"statute of limitations\"],\n",
        "        \"fraud\": [\"fraud\", \"fraudulent\", \"misrepresentation\", \"false claim\", \"false statement\"],\n",
        "        \"terms of service\": [\"terms of service\", \"terms and conditions\", \"agreement terms\", \"contract terms\"],\n",
        "        \"subcontractor\": [\"subcontractor\", \"subcontractors\", \"subcontract\"],\n",
        "        \"delay\": [\"delay\", \"delays\", \"excusable delay\", \"compensable delay\"],\n",
        "        \"termination\": [\"termination\", \"terminated for default\", \"terminated for convenience\"],\n",
        "        \"equitable adjustment\": [\"equitable adjustment\", \"price adjustment\", \"cost adjustment\"],\n",
        "        \"accord and satisfaction\": [\"accord and satisfaction\"],\n",
        "        \"waiver\": [\"waiver\", \"waived\"],\n",
        "        \"estoppel\": [\"estoppel\", \"equitable estoppel\", \"promissory estoppel\"],\n",
        "        \"sovereign immunity\": [\"sovereign immunity\"],\n",
        "        \"prime contractor\": [\"prime contractor\", \"general contractor\"],\n",
        "        \"liquidated damages\": [\"liquidated damages\"],\n",
        "        \"cure notice\": [\"cure notice\"],\n",
        "        \"dispute\": [\"dispute\", \"controversy\"],\n",
        "        \"appeal\": [\"appeal\", \"appealed\"],\n",
        "        \"certified claim\": [\"certified claim\"],\n",
        "        \"accord and satisfaction\": [\"accord and satisfaction\"],\n",
        "        \"construction\": [\"construction\", \"construct\", \"building\"],\n",
        "        \"commercial\": [\"commercial\", \"commerce\", \"business\"]\n",
        "    }\n",
        "\n",
        "    for class_name, terms in keywords.items():\n",
        "        for term in terms:\n",
        "            if term in text_to_classify:\n",
        "                classification = class_name\n",
        "                break\n",
        "        if classification != \"Unclassified\":\n",
        "            break\n",
        "\n",
        "    case_data.classification = classification\n",
        "\n",
        "def main():\n",
        "    # --- Use already downloaded PDFs ---\n",
        "    download_dir = 'downloaded_pdfs'\n",
        "    if not os.path.exists(download_dir):\n",
        "        print(f\"Error: Directory '{download_dir}' not found. Please run the scraping and downloading script first.\")\n",
        "        return\n",
        "\n",
        "    # --- Extract data from pdfs ---\n",
        "    all_cases_data = []\n",
        "    patterns = {\n",
        "        \"case_name\": r\"Case Name:\\s*(.*?)\\n\",\n",
        "        \"judges\": r\"Judges:\\s*(.*?)\\n\",\n",
        "    }\n",
        "\n",
        "    for filename in os.listdir(download_dir):\n",
        "        if filename.endswith('.pdf'):\n",
        "            file_path = os.path.join(download_dir, filename)\n",
        "            text = \"\"\n",
        "            try:\n",
        "                with open(file_path, 'rb') as f:\n",
        "                    reader = PyPDF2.PdfReader(f)\n",
        "                    for page_num in range(len(reader.pages)):\n",
        "                        text += reader.pages[page_num].extract_text()\n",
        "\n",
        "                extracted_data = {\"file_name\": filename}\n",
        "\n",
        "                for section, pattern in patterns.items():\n",
        "                    match = re.search(pattern, text, re.DOTALL)\n",
        "                    if match:\n",
        "                         extracted_data[section] = match.group(1).strip()\n",
        "                    else:\n",
        "                        extracted_data[section] = None\n",
        "\n",
        "                full_text_match = re.search(r\"Judges:.*?\\n(.*)\", text, re.DOTALL)\n",
        "                if not full_text_match:\n",
        "                     full_text_match = re.search(r\"Case Name:.*?\\n(.*)\", text, re.DOTALL)\n",
        "\n",
        "                if full_text_match:\n",
        "                    extracted_data[\"full_text\"] = full_text_match.group(1).strip()\n",
        "                else:\n",
        "                    extracted_data[\"full_text\"] = text.strip()\n",
        "\n",
        "\n",
        "                all_cases_data.append(CaseData(**extracted_data))\n",
        "\n",
        "            except PyPDF2.errors.PdfReadError:\n",
        "                print(f\"Could not read PDF file {filename}. Skipping.\")\n",
        "                all_cases_data.append(CaseData(file_name=filename, classification=\"Unreadable PDF\"))\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {filename}: {e}\")\n",
        "                all_cases_data.append(CaseData(file_name=filename, classification=f\"Processing Error: {e}\"))\n",
        "\n",
        "    print(f\"Processed {len(all_cases_data)} PDF files.\")\n",
        "\n",
        "    # --- Keyword-based classification ---\n",
        "    for case in all_cases_data:\n",
        "        classify_case(case)\n",
        "    print(f\"Classified {len(all_cases_data)} cases.\")\n",
        "\n",
        "    # --- Identify potential additional classifiers ---\n",
        "    all_text_for_analysis = \"\"\n",
        "    for case in all_cases_data:\n",
        "        if case.case_name:\n",
        "            all_text_for_analysis += case.case_name + \" \"\n",
        "        if case.full_text:\n",
        "            all_text_for_analysis += case.full_text + \" \"\n",
        "\n",
        "    tokens = all_text_for_analysis.split()\n",
        "    tokens = [word.lower().translate(str.maketrans('', '', string.punctuation)) for word in tokens]\n",
        "    tokens = [word for word in tokens if word]\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
        "    word_frequencies = Counter(filtered_tokens)\n",
        "    most_common_words = word_frequencies.most_common(50)\n",
        "\n",
        "    print(\"\\nPotential additional classifiers and their frequencies:\")\n",
        "    for word, frequency in most_common_words:\n",
        "        print(f\"{word}: {frequency}\")\n",
        "\n",
        "    # --- Structure the Data ---\n",
        "    structured_data_list = []\n",
        "    for case in all_cases_data:\n",
        "        structured_data_list.append({\n",
        "            \"file_name\": case.file_name,\n",
        "            \"case_name\": case.case_name,\n",
        "            \"judges\": case.judges,\n",
        "            \"full_text\": case.full_text,\n",
        "            \"classification\": case.classification\n",
        "        })\n",
        "    print(f\"\\nCreated a structured list containing data for {len(structured_data_list)} cases.\")\n",
        "\n",
        "    # --- Save Data to CSV ---\n",
        "    output_csv_file = '/content/case_data2.csv'\n",
        "    with open(output_csv_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "        fieldnames = [\"file_name\", \"case_name\", \"judges\", \"full_text\", \"classification\"]\n",
        "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "        writer.writeheader()\n",
        "        for case_data in structured_data_list:\n",
        "            writer.writerow(case_data)\n",
        "    print(f\"Successfully saved data for {len(structured_data_list)} cases to {output_csv_file}\")\n",
        "\n",
        "    # --- Save Data to JSON ---\n",
        "    output_json_file = '/content/case_data2.json'\n",
        "    with open(output_json_file, 'w', encoding='utf-8') as jsonfile:\n",
        "        json.dump(structured_data_list, jsonfile, indent=4)\n",
        "    print(f\"Successfully saved data for {len(structured_data_list)} cases to {output_json_file}\")\n",
        "\n",
        "    # --- Save Data to a third format (Pickle) ---\n",
        "    output_pickle_file = '/content/case_data2.pkl'\n",
        "    with open(output_pickle_file, 'wb') as pklfile:\n",
        "        pickle.dump(structured_data_list, pklfile)\n",
        "    print(f\"Successfully saved data for {len(structured_data_list)} cases to {output_pickle_file}\")\n",
        "\n",
        "    # --- Save Data to TXT ---\n",
        "    output_txt_file = '/content/case_data2.txt'\n",
        "    with open(output_txt_file, 'w', encoding='utf-8') as txtfile:\n",
        "        for case_data in structured_data_list:\n",
        "            txtfile.write(f\"File Name: {case_data.get('file_name', 'N/A')}\\n\")\n",
        "            txtfile.write(f\"Case Name: {case_data.get('case_name', 'N/A')}\\n\")\n",
        "            txtfile.write(f\"Judges: {case_data.get('judges', 'N/A')}\\n\")\n",
        "            txtfile.write(f\"Classification: {case_data.get('classification', 'N/A')}\\n\")\n",
        "            txtfile.write(\"--- Full Text ---\\n\")\n",
        "            txtfile.write(f\"{case_data.get('full_text', 'N/A')}\\n\\n\")\n",
        "            txtfile.write(\"-\" * 50 + \"\\n\\n\")\n",
        "    print(f\"Successfully saved data for {len(structured_data_list)} cases to {output_txt_file}\")\n",
        "\n",
        "    # --- Save Data to XML ---\n",
        "    output_xml_file = '/content/case_data2.xml'\n",
        "    root = ET.Element('cases')\n",
        "    for case_data in structured_data_list:\n",
        "        case_element = ET.SubElement(root, 'case')\n",
        "        for field in [\"file_name\", \"case_name\", \"judges\", \"full_text\", \"classification\"]:\n",
        "            sub_element = ET.SubElement(case_element, field)\n",
        "            sub_element.text = str(case_data.get(field, 'N/A'))\n",
        "    tree = ET.ElementTree(root)\n",
        "    with open(output_xml_file, 'w', encoding='utf-8') as xmlfile:\n",
        "        xml_string = ET.tostring(root, encoding='unicode', xml_declaration=True)\n",
        "        xmlfile.write(xml_string)\n",
        "    print(f\"Successfully saved data for {len(structured_data_list)} cases to {output_xml_file}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 1106 PDF files.\n",
            "Classified 1106 cases.\n",
            "\n",
            "Potential additional classifiers and their frequencies:\n",
            "contract: 29846\n",
            "ent: 19828\n",
            "claim: 17652\n",
            "board: 15011\n",
            "v: 12811\n",
            "cbca: 11793\n",
            "contracting: 11453\n",
            "appeal: 10759\n",
            "contractor: 9959\n",
            "work: 9882\n",
            "appellant: 9873\n",
            "exhibit: 9420\n",
            "f: 9024\n",
            "would: 8959\n",
            "costs: 8543\n",
            ": 8437\n",
            "officer: 8402\n",
            "decision: 8380\n",
            "gsa: 7731\n",
            "services: 7604\n",
            "bca: 7479\n",
            "id: 7343\n",
            "states: 7290\n",
            "mr: 7039\n",
            "inc: 7014\n",
            "1: 6906\n",
            "order: 6191\n",
            "motion: 6143\n",
            "2: 6063\n",
            "united: 5909\n",
            "may: 5489\n",
            "also: 5332\n",
            "respondent: 5247\n",
            "governm: 5203\n",
            "fed: 5106\n",
            "time: 5101\n",
            "va: 5003\n",
            "agency: 4884\n",
            "ents: 4873\n",
            "required: 4837\n",
            "see: 4779\n",
            "3: 4771\n",
            "parties: 4762\n",
            "amount: 4739\n",
            "general: 4700\n",
            "construction: 4649\n",
            "government: 4644\n",
            "e: 4600\n",
            "judge: 4524\n",
            "cir: 4450\n",
            "\n",
            "Created a structured list containing data for 1106 cases.\n",
            "Successfully saved data for 1106 cases to /content/case_data2.csv\n",
            "Successfully saved data for 1106 cases to /content/case_data2.json\n",
            "Successfully saved data for 1106 cases to /content/case_data2.pkl\n",
            "Successfully saved data for 1106 cases to /content/case_data2.txt\n",
            "Successfully saved data for 1106 cases to /content/case_data2.xml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b362ac61"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Process Overview:\n",
        "\n",
        "1.  **Data Loading**: The script started by checking for the presence of the `downloaded_pdfs` directory, confirming it will use the already downloaded files.\n",
        "2.  **Data Extraction**: The script iterated through the downloaded PDFs in the `downloaded_pdfs` directory, extracted the full text content of each document, and identified the case name and judges.\n",
        "3.  **Keyword-Based Classification**: Each case was classified based on the presence of a refined list of keywords related to common CBCA contract appeal terms found within the full text, case name, and file name. The classifiers used include: \"Dismissal\", \"Decision\", \"Summary Judgment\", \"Order\", \"Judge\", \"Jurisdiction\", \"Site condition\", \"Christian\", \"breach of contract\", \"breach of duty of good faith and fair dealing\", \"bankrupt\", \"government claim\", \"untimely\", \"fraud\", \"terms of service\", \"subcontractor\", \"delay\", \"delays\", \"excusable delay\", \"compensable delay\", \"termination\", \"terminated for default\", \"terminated for convenience\", \"equitable adjustment\", \"price adjustment\", \"cost adjustment\", \"accord and satisfaction\", \"waiver\", \"waived\", \"estoppel\", \"equitable estoppel\", \"promissory estoppel\", \"sovereign immunity\", \"prime contractor\", \"general contractor\", \"liquidated damages\", \"cure notice\", \"dispute\", \"controversy\", \"appeal\", \"appealed\", \"certified claim\", \"construction\", and \"commercial\".\n",
        "4.  **Potential Additional Classifiers**: A frequency analysis of words in the extracted text was performed to identify potential additional classifiers based on the content of the processed PDFs.\n",
        "5.  **Data Structuring**: The extracted full text, along with the case name, judges, and classification, was organized into a structured list of dictionaries.\n",
        "6.  **Data Saving**: The structured data was successfully saved into five different formats with updated filenames in the `/content/` directory:\n",
        "    *   CSV (`/content/case_data2.csv`)\n",
        "    *   JSON (`/content/case_data2.json`)\n",
        "    *   Pickle (`/content/case_data2.pkl`)\n",
        "    *   TXT (`/content/case_data2.txt`)\n",
        "    *   XML (`/content/case_data2.xml`)\n",
        "\n",
        "### Output Files:\n",
        "\n",
        "*   `/content/case_data2.csv`: Contains the structured data in comma-separated values format.\n",
        "*   `/content/case_data2.json`: Contains the structured data in JSON format.\n",
        "*   `/content/case_data2.pkl`: Contains the structured data in Pickle format, preserving Python object structure.\n",
        "*   `/content/case_data2.txt`: Contains the structured data in a human-readable text format with sections clearly delineated.\n",
        "*   `/content/case_data2.xml`: Contains the structured data in XML format.\n",
        "\n",
        "### Potential Additional Classifiers Identified:\n",
        "\n",
        "The frequency analysis of the text identified several frequently occurring terms that could be considered as additional classifiers. You can review the output from the code execution for the full list and their frequencies.\n",
        "\n",
        "### Next Steps:\n",
        "\n",
        "*   **Review Potential Classifiers:** Evaluate the identified potential classifiers from the frequency analysis to determine their relevance and usefulness for refining or expanding the classification system.\n",
        "*   **Utilize Structured Data:** The generated CSV, JSON, Pickle, TXT, and XML files are now available in the `/content/` folder and can be used for various AI tasks, such as training models, data analysis, or building applications.\n",
        "*   **Refine Classification Logic:** Based on the analysis of the extracted data and the identified classifiers, you might want to further refine the classification logic for improved accuracy or explore more advanced text analysis techniques."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 844
        },
        "id": "4830a6a5",
        "outputId": "74eac991-5307-4f2e-9579-bbbc54f09b75"
      },
      "source": [
        "import os\n",
        "import time\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin\n",
        "import PyPDF2\n",
        "import re\n",
        "import csv\n",
        "import json\n",
        "import pickle\n",
        "import xml.etree.ElementTree as ET\n",
        "import string\n",
        "from collections import Counter\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "\n",
        "# Download stopwords if not already downloaded\n",
        "try:\n",
        "    stopwords.words('english')\n",
        "except LookupError:\n",
        "    nltk.download('stopwords')\n",
        "    from nltk.corpus import stopwords\n",
        "\n",
        "class CaseData:\n",
        "    \"\"\"\n",
        "    A data structure to hold extracted information from a PDF case file.\n",
        "    \"\"\"\n",
        "    def __init__(self, file_name=None, case_name=None, judges=None, full_text=None,\n",
        "                 classification=None):\n",
        "        self.file_name = file_name\n",
        "        self.case_name = case_name\n",
        "        self.judges = judges\n",
        "        self.full_text = full_text\n",
        "        self.classification = classification\n",
        "\n",
        "def classify_case(case_data):\n",
        "    \"\"\"\n",
        "    Classifies a CaseData object based on keywords in its content.\n",
        "\n",
        "    Args:\n",
        "        case_data: A CaseData object.\n",
        "    \"\"\"\n",
        "    classification = \"Unclassified\"\n",
        "    text_to_classify = \"\"\n",
        "\n",
        "    if case_data.full_text:\n",
        "        text_to_classify += case_data.full_text.lower()\n",
        "    if case_data.case_name:\n",
        "        text_to_classify += case_data.case_name.lower()\n",
        "    if case_data.file_name:\n",
        "        text_to_classify += case_data.file_name.lower()\n",
        "\n",
        "    keywords = {\n",
        "        \"Dismissal\": [\"dismissal\", \"motion to dismiss\"],\n",
        "        \"Decision\": [\"decision\", \"summary judgment\"],\n",
        "        \"Order\": [\"order\"],\n",
        "        \"Judge\": [\"judge\", \"judges\", \"justice\", \"administrative judge\"],\n",
        "        \"Jurisdiction\": [\"jurisdiction\", \"subject matter jurisdiction\", \"lack of jurisdiction\", \"venue\", \"authority\"],\n",
        "        \"Site condition\": [\"site condition\", \"site conditions\", \"differing site condition\", \"differing site conditions\", \"changed condition\", \"changed conditions\"],\n",
        "        \"Christian\": [\"christian\"],\n",
        "        \"breach of contract\": [\"breach of contract\", \"breach of the agreement\", \"violation of contract\"],\n",
        "        \"breach of duty of good faith and fair dealing\": [\"breach of duty of good faith and fair dealing\"],\n",
        "        \"bankrupt\": [\"bankrupt\", \"bankruptcy\", \"insolvent\", \"receivership\"],\n",
        "        \"government claim\": [\"government claim\", \"government claims\", \"claim against the government\", \"contract dispute\", \"contract claim\"],\n",
        "        \"untimely\": [\"untimely\", \"late\", \"time-barred\", \"statute of limitations\"],\n",
        "        \"fraud\": [\"fraud\", \"fraudulent\", \"misrepresentation\", \"false claim\", \"false statement\"],\n",
        "        \"terms of service\": [\"terms of service\", \"terms and conditions\", \"agreement terms\", \"contract terms\"],\n",
        "        \"subcontractor\": [\"subcontractor\", \"subcontractors\", \"subcontract\"],\n",
        "        \"delay\": [\"delay\", \"delays\", \"excusable delay\", \"compensable delay\"],\n",
        "        \"termination\": [\"termination\", \"terminated for default\", \"terminated for convenience\"],\n",
        "        \"equitable adjustment\": [\"equitable adjustment\", \"price adjustment\", \"cost adjustment\"],\n",
        "        \"accord and satisfaction\": [\"accord and satisfaction\"],\n",
        "        \"waiver\": [\"waiper\", \"waived\"],\n",
        "        \"estoppel\": [\"estoppel\", \"equitable estoppel\", \"promissory estoppel\"],\n",
        "        \"sovereign immunity\": [\"sovereign immunity\"],\n",
        "        \"prime contractor\": [\"prime contractor\", \"general contractor\"],\n",
        "        \"liquidated damages\": [\"liquidated damages\"],\n",
        "        \"cure notice\": [\"cure notice\"],\n",
        "        \"dispute\": [\"dispute\", \"controversy\"],\n",
        "        \"appeal\": [\"appeal\", \"appealed\"],\n",
        "        \"certified claim\": [\"certified claim\"],\n",
        "        \"accord and satisfaction\": [\"accord and satisfaction\"],\n",
        "        \"construction\": [\"construction\", \"construct\", \"building\"],\n",
        "        \"commercial\": [\"commercial\", \"commerce\", \"business\"]\n",
        "    }\n",
        "\n",
        "    for class_name, terms in keywords.items():\n",
        "        for term in terms:\n",
        "            if term in text_to_classify:\n",
        "                classification = class_name\n",
        "                break\n",
        "        if classification != \"Unclassified\":\n",
        "            break\n",
        "\n",
        "    case_data.classification = classification\n",
        "\n",
        "def main():\n",
        "    # --- Web scraping ---\n",
        "    url = \"https://cbca.gov/decisions/cda-cases.html\"\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "        pdf_links = []\n",
        "        for a_tag in soup.find_all('a', href=True):\n",
        "            href = a_tag['href']\n",
        "            if href.lower().endswith('.pdf'):\n",
        "                pdf_links.append(href)\n",
        "        print(f\"Found {len(pdf_links)} PDF links.\")\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error during request: {e}\")\n",
        "        return # Exit if scraping fails\n",
        "\n",
        "    # --- PDF downloading ---\n",
        "    download_dir = 'downloaded_pdfs'\n",
        "    if not os.path.exists(download_dir):\n",
        "        os.makedirs(download_dir)\n",
        "        print(f\"Created directory: {download_dir}\")\n",
        "    else:\n",
        "        print(f\"Directory already exists: {download_dir}\")\n",
        "\n",
        "    base_url = \"https://cbca.gov/decisions/\"\n",
        "    download_count = 0\n",
        "\n",
        "    for link in pdf_links:\n",
        "        full_url = urljoin(base_url, link)\n",
        "        filename = os.path.join(download_dir, os.path.basename(full_url))\n",
        "\n",
        "        if not os.path.exists(filename): # Skip download if file already exists\n",
        "            try:\n",
        "                response = requests.get(full_url, stream=True)\n",
        "                response.raise_for_status()\n",
        "\n",
        "                temp_filename = filename + \".temp\"\n",
        "                with open(temp_filename, 'wb') as f:\n",
        "                    for chunk in response.iter_content(chunk_size=8192):\n",
        "                        f.write(chunk)\n",
        "\n",
        "                try:\n",
        "                    with open(temp_filename, 'rb') as f:\n",
        "                        reader = PyPDF2.PdfReader(f)\n",
        "                        num_pages = len(reader.pages)\n",
        "\n",
        "                    if num_pages <= 2:\n",
        "                        print(f\"Skipping {full_url} as it has only {num_pages} page(s).\")\n",
        "                        os.remove(temp_filename)\n",
        "                        continue\n",
        "                    else:\n",
        "                        os.rename(temp_filename, filename)\n",
        "                        print(f\"Successfully downloaded: {filename} with {num_pages} pages.\")\n",
        "                        download_count += 1\n",
        "\n",
        "                except PyPDF2.errors.PdfReadError:\n",
        "                     print(f\"Could not read PDF file {full_url}. Skipping.\")\n",
        "                     os.remove(temp_filename)\n",
        "                     continue\n",
        "\n",
        "                if download_count > 0 and download_count % 10 == 0:\n",
        "                    print(\"Pausing for 10 seconds for rate limiting...\")\n",
        "                    time.sleep(10)\n",
        "\n",
        "            except requests.exceptions.RequestException as e:\n",
        "                print(f\"Error downloading {full_url}: {e}\")\n",
        "            except IOError as e:\n",
        "                print(f\"Error saving file {filename}: {e}\")\n",
        "        else:\n",
        "            print(f\"File already exists: {filename}. Skipping download.\")\n",
        "\n",
        "\n",
        "    # --- Extract data from pdfs ---\n",
        "    all_cases_data = []\n",
        "    # Simplified patterns to only get case name and judges, then capture the rest as full text\n",
        "    patterns = {\n",
        "        \"case_name\": r\"Case Name:\\s*(.*?)\\n\",\n",
        "        \"judges\": r\"Judges:\\s*(.*?)\\n\",\n",
        "    }\n",
        "\n",
        "\n",
        "    for filename in os.listdir(download_dir):\n",
        "        if filename.endswith('.pdf'):\n",
        "            file_path = os.path.join(download_dir, filename)\n",
        "            text = \"\"\n",
        "            try:\n",
        "                with open(file_path, 'rb') as f:\n",
        "                    reader = PyPDF2.PdfReader(f)\n",
        "                    for page_num in range(len(reader.pages)):\n",
        "                        text += reader.pages[page_num].extract_text()\n",
        "\n",
        "                extracted_data = {\"file_name\": filename}\n",
        "\n",
        "                # Extract case name and judges\n",
        "                for section, pattern in patterns.items():\n",
        "                    match = re.search(pattern, text, re.DOTALL)\n",
        "                    if match:\n",
        "                         extracted_data[section] = match.group(1).strip()\n",
        "                    else:\n",
        "                        extracted_data[section] = None\n",
        "\n",
        "                # Capture the full text after judges (or after case name if no judges found)\n",
        "                full_text_match = re.search(r\"Judges:.*?\\n(.*)\", text, re.DOTALL)\n",
        "                if not full_text_match:\n",
        "                     full_text_match = re.search(r\"Case Name:.*?\\n(.*)\", text, re.DOTALL)\n",
        "\n",
        "                if full_text_match:\n",
        "                    extracted_data[\"full_text\"] = full_text_match.group(1).strip()\n",
        "                else:\n",
        "                    extracted_data[\"full_text\"] = text.strip() # Fallback to entire text if patterns fail\n",
        "\n",
        "\n",
        "                all_cases_data.append(CaseData(**extracted_data))\n",
        "\n",
        "            except PyPDF2.errors.PdfReadError:\n",
        "                print(f\"Could not read PDF file {filename}. Skipping.\")\n",
        "                all_cases_data.append(CaseData(file_name=filename, classification=\"Unreadable PDF\"))\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {filename}: {e}\")\n",
        "                all_cases_data.append(CaseData(file_name=filename, classification=f\"Processing Error: {e}\"))\n",
        "\n",
        "    print(f\"Processed {len(all_cases_data)} PDF files.\")\n",
        "\n",
        "    # --- Keyword-based classification ---\n",
        "    for case in all_cases_data:\n",
        "        classify_case(case)\n",
        "    print(f\"Classified {len(all_cases_data)} cases.\")\n",
        "\n",
        "    # --- Identify potential additional classifiers ---\n",
        "    all_text_for_analysis = \"\"\n",
        "    for case in all_cases_data:\n",
        "        if case.case_name:\n",
        "            all_text_for_analysis += case.case_name + \" \"\n",
        "        if case.full_text:\n",
        "            all_text_for_analysis += case.full_text + \" \"\n",
        "\n",
        "\n",
        "    tokens = all_text_for_analysis.split()\n",
        "    tokens = [word.lower().translate(str.maketrans('', '', string.punctuation)) for word in tokens]\n",
        "    tokens = [word for word in tokens if word]\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
        "    word_frequencies = Counter(filtered_tokens)\n",
        "    most_common_words = word_frequencies.most_common(50)\n",
        "\n",
        "    print(\"\\nPotential additional classifiers and their frequencies:\")\n",
        "    for word, frequency in most_common_words:\n",
        "        print(f\"{word}: {frequency}\")\n",
        "\n",
        "    # --- Structure the Data ---\n",
        "    structured_data_list = []\n",
        "    for case in all_cases_data:\n",
        "        structured_data_list.append({\n",
        "            \"file_name\": case.file_name,\n",
        "            \"case_name\": case.case_name,\n",
        "            \"judges\": case.judges,\n",
        "            \"full_text\": case.full_text,\n",
        "            \"classification\": case.classification\n",
        "        })\n",
        "    print(f\"\\nCreated a structured list containing data for {len(structured_data_list)} cases.\")\n",
        "\n",
        "\n",
        "    # --- Save Data to CSV ---\n",
        "    output_csv_file = '/content/case_data2.csv'\n",
        "    with open(output_csv_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "        fieldnames = [\"file_name\", \"case_name\", \"judges\", \"full_text\", \"classification\"]\n",
        "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "        writer.writeheader()\n",
        "        for case_data in structured_data_list:\n",
        "            writer.writerow(case_data)\n",
        "    print(f\"Successfully saved data for {len(structured_data_list)} cases to {output_csv_file}\")\n",
        "\n",
        "    # --- Save Data to JSON ---\n",
        "    output_json_file = '/content/case_data2.json'\n",
        "    with open(output_json_file, 'w', encoding='utf-8') as jsonfile:\n",
        "        json.dump(structured_data_list, jsonfile, indent=4)\n",
        "    print(f\"Successfully saved data for {len(structured_data_list)} cases to {output_json_file}\")\n",
        "\n",
        "    # --- Save Data to a third format (Pickle) ---\n",
        "    output_pickle_file = '/content/case_data2.pkl'\n",
        "    with open(output_pickle_file, 'wb') as pklfile:\n",
        "        pickle.dump(structured_data_list, pklfile)\n",
        "    print(f\"Successfully saved data for {len(structured_data_list)} cases to {output_pickle_file}\")\n",
        "\n",
        "    # --- Save Data to TXT ---\n",
        "    output_txt_file = '/content/case_data2.txt'\n",
        "    with open(output_txt_file, 'w', encoding='utf-8') as txtfile:\n",
        "        for case_data in structured_data_list:\n",
        "            txtfile.write(f\"File Name: {case_data.get('file_name', 'N/A')}\\n\")\n",
        "            txtfile.write(f\"Case Name: {case_data.get('case_name', 'N/A')}\\n\")\n",
        "            txtfile.write(f\"Judges: {case_data.get('judges', 'N/A')}\\n\")\n",
        "            txtfile.write(f\"Classification: {case_data.get('classification', 'N/A')}\\n\")\n",
        "            txtfile.write(\"--- Full Text ---\\n\")\n",
        "            txtfile.write(f\"{case_data.get('full_text', 'N/A')}\\n\\n\")\n",
        "            txtfile.write(\"-\" * 50 + \"\\n\\n\")\n",
        "    print(f\"Successfully saved data for {len(structured_data_list)} cases to {output_txt_file}\")\n",
        "\n",
        "    # --- Save Data to XML ---\n",
        "    output_xml_file = '/content/case_data2.xml'\n",
        "    root = ET.Element('cases')\n",
        "    for case_data in structured_data_list:\n",
        "        case_element = ET.SubElement(root, 'case')\n",
        "        for field in [\"file_name\", \"case_name\", \"judges\", \"full_text\", \"classification\"]:\n",
        "            sub_element = ET.SubElement(case_element, field)\n",
        "            sub_element.text = str(case_data.get(field, 'N/A'))\n",
        "    tree = ET.ElementTree(root)\n",
        "    with open(output_xml_file, 'w', encoding='utf-8') as xmlfile:\n",
        "        xml_string = ET.tostring(root, encoding='unicode', xml_declaration=True)\n",
        "        xmlfile.write(xml_string)\n",
        "    print(f\"Successfully saved data for {len(structured_data_list)} cases to {output_xml_file}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 3435 PDF links.\n",
            "Directory already exists: downloaded_pdfs\n",
            "File already exists: downloaded_pdfs/RUSSELL_08-18-25_8346__REAGENT_WORLD_INC (DISMISSAL).pdf. Skipping download.\n",
            "File already exists: downloaded_pdfs/RUSSELL_08-19-25_6198__EAGLE_PEAK_ROCK_AND_PAVING_INC (DISMISSAL).pdf. Skipping download.\n",
            "File already exists: downloaded_pdfs/RUSSELL_08-19-25_7832(5692)-REM-R__EAGLE_PEAK_ROCK_AND_PAVING_INC (DISMISSAL).pdf. Skipping download.\n",
            "File already exists: downloaded_pdfs/RUSSELL_08-19-25_8456__GAM3_CONSTRUCTION_LLC (DISMISSAL).pdf. Skipping download.\n",
            "File already exists: downloaded_pdfs/SULLIVAN_08-18-25_7451-R__QUALITY_TRUST_INC (DECISION).pdf. Skipping download.\n",
            "Skipping https://cbca.gov/files/decisions/2025/KULLBERG_08-07-25_8222, 8424__HERNANDEZ_CONSULTING_INC_DBA (DISMISSAL).pdf as it has only 1 page(s).\n",
            "File already exists: downloaded_pdfs/RUSSELL_07-31-25_8435__MISSOURI_HIGHER_EDUCATION_LOAN_AUTHORITY.pdf. Skipping download.\n",
            "Skipping https://cbca.gov/files/decisions/2025/VERGILIO_07-30-25_8415__SALVADORINI_CONSULTING_LLC (DISMISSAL).pdf as it has only 1 page(s).\n",
            "Skipping https://cbca.gov/files/decisions/2025/CHADWICK_07-29-25_8270__HERNANDEZ_CONSULTING_INC_DBA_HERNANDEZ_CONSULTING_AND_CONSTRUCTION (DISMISSAL).pdf as it has only 1 page(s).\n",
            "Skipping https://cbca.gov/files/decisions/2025/ZISCHKAU_07-29-25_8266__THE_SPANOS_GROUP_LLC (DECISION).pdf as it has only 2 page(s).\n",
            "Skipping https://cbca.gov/files/decisions/2025/GOODMAN_07-22-25_8445__HERMAN_CONSTRUCTION_GROUP_INC (DISMISSAL).pdf as it has only 2 page(s).\n",
            "File already exists: downloaded_pdfs/KANG_07-18-25_8000__TROOP_CONTRACTING_INC (DECISION).pdf. Skipping download.\n",
            "Skipping https://cbca.gov/files/decisions/2025/SULLIVAN_07-23-25_8119__MISSOURI_HIGHER_EDUCATION_LOAN_AUTHORITY (DECISION).pdf as it has only 2 page(s).\n",
            "File already exists: downloaded_pdfs/VERGILIO_07-25-25_8012__DELTA_T_JR_LLC (DECISION).pdf. Skipping download.\n",
            "Skipping https://cbca.gov/files/decisions/2025/VOLK_07-18-25_8463__NUES_INC (DISMISSAL).pdf as it has only 1 page(s).\n",
            "File already exists: downloaded_pdfs/LESTER_07-25-25_8431__TRIBAL_HEALTH_LLC (DECISION).pdf. Skipping download.\n",
            "Skipping https://cbca.gov/files/decisions/2025/KANG_07-16-25_8466__FAITH_ENTERPRISES_INCORPORATED (DISMISSAL).pdf as it has only 2 page(s).\n",
            "Skipping https://cbca.gov/files/decisions/2025/GOODMAN_07-11-25_8047(6814)-REM__CARING_HANDS_HEALTH_EQUIPMENT_AND_SUPPLIES_LLC (DECISION).pdf as it has only 2 page(s).\n",
            "File already exists: downloaded_pdfs/KULLBERG_07-11-25_8335__OUTSIDE_THE_BOX_LLC (DECISION).pdf. Skipping download.\n",
            "Skipping https://cbca.gov/files/decisions/2025/SHERIDAN_07-10-25_8283__BARNARD_SPENCER_JOINT_VENTURE (DECISION).pdf as it has only 2 page(s).\n",
            "Skipping https://cbca.gov/files/decisions/2025/ZISCHKAU_07-11-25_8081__CONCIERGE_BUSINESS_SOLUTIONS_INC (DISMISSAL).pdf as it has only 2 page(s).\n",
            "Skipping https://cbca.gov/files/decisions/2025/SHERIDAN_07-07-25_8313__CADDELL_CONSTRUCTION_DE_LLC (DECISION).pdf as it has only 2 page(s).\n",
            "Skipping https://cbca.gov/files/decisions/2025/SHERIDAN_07-07-25_8314__CADDELL_CONSTRUCTION_DE_LLC (DECISION).pdf as it has only 2 page(s).\n",
            "Skipping https://cbca.gov/files/decisions/2025/CHADWICK_06-26-25_7864__BURHAN_SECURITY_SERVICES (DISMISSAL).pdf as it has only 1 page(s).\n",
            "Skipping https://cbca.gov/files/decisions/2025/OROURKE_06-27-25_7774__SEA_PAC_ENGINEERING_INC (DISMISSAL).pdf as it has only 1 page(s).\n",
            "File already exists: downloaded_pdfs/OROURKE_06-30-25_8366__UNLIMITCOMP_LLC (DECISION).pdf. Skipping download.\n",
            "Skipping https://cbca.gov/files/decisions/2025/RUSSELL_06-27-25_8414__VAF_100_DAWSON_DRIVE_LLC (DISMISSAL).pdf as it has only 1 page(s).\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1082455853.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-1082455853.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# Skip download if file already exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m                 \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/requests/api.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \"\"\"\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"get\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    587\u001b[0m         }\n\u001b[1;32m    588\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 703\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    704\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 667\u001b[0;31m             resp = conn.urlopen(\n\u001b[0m\u001b[1;32m    668\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m                 \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m             \u001b[0;31m# Make the request on the HTTPConnection object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m             response = self._make_request(\n\u001b[0m\u001b[1;32m    788\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    462\u001b[0m             \u001b[0;31m# Trigger any extra validation we need to do.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 464\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    465\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_timeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m   1091\u001b[0m         \u001b[0;31m# Force connect early to allow us to validate the connection.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1092\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_closed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1093\u001b[0;31m             \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1094\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1095\u001b[0m         \u001b[0;31m# TODO revise this, see https://github.com/urllib3/urllib3/issues/2791\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    751\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    752\u001b[0m             \u001b[0msock\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mssl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSSLSocket\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 753\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    754\u001b[0m             \u001b[0mserver_hostname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    755\u001b[0m             \u001b[0mtls_in_tls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36m_new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    196\u001b[0m         \"\"\"\n\u001b[1;32m    197\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 198\u001b[0;31m             sock = connection.create_connection(\n\u001b[0m\u001b[1;32m    199\u001b[0m                 \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dns_host\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/urllib3/util/connection.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msource_address\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m                 \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_address\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m             \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msa\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m             \u001b[0;31m# Break explicitly a reference cycle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0merr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}